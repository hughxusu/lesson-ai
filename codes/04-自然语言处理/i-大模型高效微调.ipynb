{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3418a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'95a8e8fdd41cdc4b\\r\\nx-scheme: https\\r\\nx-request-id: 70e4d268482fa4f027007ded9023f539\\r\\nx-real-ip: 111']\n",
      "Bad pipe message: %s [b'99.82.105\\r\\nx-original-forwarded-for: 111.199.', b'.105\\r\\nx-forwarded-scheme: https\\r\\nx-forwarded-proto: htt']\n",
      "Bad pipe message: %s [b',http\\r\\nx-forwarded-port: 443,80\\r\\nx-forwarded-host: 1fcab7d27eec46b895a8e8fdd41cdc4b--45085.ap-shanghai.cloudstu', b'o.club\\r\\nx-forwarded-for: 111.199.82.105, 172.16.5.13, 172.18.138.4, 172.17.83.15,::ffff:10.89.0.2\\r\\n', b'client-proto-ver: HTTP/2.0\\r\\nx-client-proto: https\\r\\nupgrade-insecure-requests: 1\\r\\nsec-fetch-storage-access: active\\r\\nsec-', b'tch-site: same-site\\r\\nsec-fetch-mode: navigate\\r\\nsec-fetch-dest: iframe\\r\\nsec-ch-ua-platform: \"macOS\"\\r\\ns', b'-ch-ua-mobile: ?0\\r\\nsec-ch-ua: \"Not;A=Brand\";v=\"99\", \"Google Chrome\";v=\"139\", \"Chromium\";v=\"139\"\\r\\nref', b'er: https://1fcab7d27eec46b895a8e8fdd41cdc4b.ap-shanghai.cloudstudio.club/\\r\\npriority: u=0, i\\r\\naccept']\n",
      "Bad pipe message: %s [b'anguage: zh-CN,zh;q=0.9,en;q=0.8\\r\\naccept-enc']\n",
      "Bad pipe message: %s [b'ing: gzip, deflate, br, zstd\\r\\naccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/w']\n",
      "Bad pipe message: %s [b'p,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\\r\\nuser-agent: Mozilla/5.0 (Macintosh; I']\n",
      "Bad pipe message: %s [b'el Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36\\r\\nhost: 1fcab7d27ee']\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea7fb9246c643298d5bcdd8bc5b11bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2b897fef144bdf8d5b8028ed1dfeec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2e0ac54c1e48bfa34bad58c9e8a2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000c5b661de3491c87a198e2631700bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5582afd35334451f98d913ce49e66698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5fc01f13654d3f8ee5b3f6d80e1f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabbcf21bd264582b4a5d02ab0137830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, OPTForCausalLM\n",
    "\n",
    "model_id = \"facebook/opt-6.7b\"\n",
    "\n",
    "model = OPTForCausalLM.from_pretrained(model_id, load_in_8bit=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9f024f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Python is the best programming language.\n",
      "I'm not sure if you're being sarcastic or not, but I'm going to assume you're being serious.                  \n"
     ]
    }
   ],
   "source": [
    "text = \"Python is the best programming language.\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)  \n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9269ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b7f45ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.80GB\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 4096, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 4096)\n",
      "      (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "            (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "            (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "            (out_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "          (fc2): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "memory_footprint_bytes = model.get_memory_footprint()\n",
    "memory_footprint_mib = memory_footprint_bytes / (1024 ** 3)  # 转换为 GB\n",
    "\n",
    "print(f\"{memory_footprint_mib:.2f}GB\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c3a4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,666,862,592 || trainable%: 0.1258\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): OPTForCausalLM(\n",
      "      (model): OPTModel(\n",
      "        (decoder): OPTDecoder(\n",
      "          (embed_tokens): Embedding(50272, 4096, padding_idx=1)\n",
      "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 4096)\n",
      "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (layers): ModuleList(\n",
      "            (0-31): 32 x OPTDecoderLayer(\n",
      "              (self_attn): OPTSdpaAttention(\n",
      "                (k_proj): lora.Linear8bitLt(\n",
      "                  (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (v_proj): lora.Linear8bitLt(\n",
      "                  (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_proj): lora.Linear8bitLt(\n",
      "                  (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_proj): lora.Linear8bitLt(\n",
      "                  (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "              (fc2): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "              (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=50272, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 创建一个LoraConfig对象，用于设置LoRA的配置参数\n",
    "config = LoraConfig(\n",
    "    r=8,            # LoRA的秩，影响LoRA矩阵的大小\n",
    "    lora_alpha=32,  # LoRA适应的比例因子\n",
    "    # 指定将LoRA应用到的模型模块，通常是attention和全连接层的投影\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc_in\", \"fc_out\"],\n",
    "    lora_dropout=0.05,     # 在LoRA模块中使用的dropout率\n",
    "    bias=\"none\",           # 设置bias的使用方式，这里没有使用bias\n",
    "    task_type=\"CAUSAL_LM\"  # 任务类型，这里设置为因果(自回归）语言模型\n",
    ")\n",
    "\n",
    "# 使用get_peft_model函数和给定的配置来获取一个PEFT模型\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff1eb109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e70dc2b1b640d1a20f18cebe0de8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d6c1f9319e4fcd80c29f0f4f7bb1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "quotes.jsonl:   0%|          | 0.00/647k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e82797aaa6641c78377b39fa735b805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['quote', 'author', 'tags'],\n",
      "    num_rows: 2508\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Abirate/english_quotes\")\n",
    "print(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df81cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e64af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>author</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“The good thing about science is that it's true whether or not you believe in it.”</td>\n",
       "      <td>Neil deGrasse Tyson</td>\n",
       "      <td>[belief, science, true]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Donâ€™t grieve. Anything you lose comes round in another form.”</td>\n",
       "      <td>Rumi</td>\n",
       "      <td>[bereavement, consolation, grief, loss, reincarnation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Religion has actually convinced people that there's an invisible man living in the sky who watches everything you do, every minute of every day. And the invisible man has a special list of ten things he does not want you to do. And if you do any of these ten things, he has a special place, full of fire and smoke and burning and torture and anguish, where he will send you to live and suffer and burn and choke and scream and cry forever and ever 'til the end of time! But He loves you. He loves you, and He needs money! He always needs money! He's all-powerful, all-perfect, all-knowing, and all-wise, somehow just can't handle money!”</td>\n",
       "      <td>George Carlin</td>\n",
       "      <td>[atheism, humor, life, religion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“Either write something worth reading or do something worth writing.”</td>\n",
       "      <td>Benjamin Franklin</td>\n",
       "      <td>[hmmm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Read, read, read. Read everything -- trash, classics, good and bad, and see how they do it. Just like a carpenter who works as an apprentice and studies the master. Read! You'll absorb it.Then write. If it's good, you'll find out. If it's not, throw it out of the window.”</td>\n",
       "      <td>William Faulkner</td>\n",
       "      <td>[reading, writing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>“The last enemy that shall be destroyed is death.”</td>\n",
       "      <td>J.K. Rowling,</td>\n",
       "      <td>[bible, death, enemy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>“I think that if I ever have kids, and they are upset, I won't tell them that people are starving in China or anything like that because it wouldn't change the fact that they were upset. And even if somebody else has it much worse, that doesn't really change the fact that you have what you have.”</td>\n",
       "      <td>Stephen Chbosky,</td>\n",
       "      <td>[honesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>“The past has no power over the present moment.”</td>\n",
       "      <td>Eckhart Tolle</td>\n",
       "      <td>[education, inspirational, life, philosophy, truth, wisdom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>“There are wounds that never show on the body that are deeper and more hurtful than anything that bleeds.”</td>\n",
       "      <td>Laurell K. Hamilton,</td>\n",
       "      <td>[depression, pain, trauma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>“Science and religion are not at odds. Science is simply too young to understand.”</td>\n",
       "      <td>Dan Brown,</td>\n",
       "      <td>[books, dan-brown, religion, science]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63c979a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca42e162f3cc43e2828f514088ebb9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenized_dataset = dataset.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b78535fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "save_path = \"./data/hf/models/opt-6.7b-lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,           # 指定模型输出和保存的目录\n",
    "    per_device_train_batch_size=4,  # 每个设备上的训练批量大小\n",
    "    learning_rate=2e-4,             # 学习率\n",
    "    fp16=True,                      # 启用混合精度训练，可以提高训练速度，同时减少内存使用\n",
    "    logging_steps=20,               # 指定日志记录的步长，用于跟踪训练进度\n",
    "    max_steps=100,                  # 最大训练步长\n",
    "    num_train_epochs=1              # 训练的总轮数\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a32d31b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.241, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                               # 指定训练时使用的模型\n",
    "    train_dataset=tokenized_dataset[\"train\"],  # 指定训练数据集\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "model.use_cache = False # 禁用模型的自回归生成缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b2876c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:56, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.808700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.800100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.954100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.8899650192260742, metrics={'train_runtime': 177.8259, 'train_samples_per_second': 2.249, 'train_steps_per_second': 0.562, 'total_flos': 1364172665978880.0, 'train_loss': 1.8899650192260742, 'epoch': 0.1594896331738437})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04726aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1889d6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is the best programming language.\n",
      ",,,,,, The The The The The.. I I am in in in in in the The The The The I am am a a a Son Son,,,....\n"
     ]
    }
   ],
   "source": [
    "lora_model = trainer.model\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "out = lora_model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "879775c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'zai-org/chatglm3-6b'          # 模型ID\n",
    "data_id = 'HasturOfficial/adgen'          # 训练数据ID\n",
    "eval_data_path = None                     # 验证数据路径，如果没有则设置为None\n",
    "seed = 8                                  # 随机种子\n",
    "max_input_length = 512                    # 输入的最大长度\n",
    "max_output_length = 1536                  # 输出的最大长度\n",
    "lora_rank = 4                             # LoRA秩\n",
    "lora_alpha = 32                           # LoRA alpha值\n",
    "lora_dropout = 0.05                       # LoRA Dropout率\n",
    "resume_from_checkpoint = None             # 如果从checkpoint恢复训练，指定路径\n",
    "prompt_text = ''                          # 所有数据前的指令文本\n",
    "compute_dtype = 'fp32'                    # 计算数据类型（fp32, fp16, bf16）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82bba622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['content', 'summary'],\n",
      "        num_rows: 114599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['content', 'summary'],\n",
      "        num_rows: 1070\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(data_id)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "063bf944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>类型#裤*版型#立体剪裁*风格#简约*风格#职场*裤长#连体裤*裤型#连衣裤</td>\n",
       "      <td>这款简约优雅的连衣裤，省去穿搭的烦恼，一身的粉嫩装扮冲撞了连体裤的霸气，更显现出优雅的小女人范儿。立体剪裁的腰线设计，凸显腰身打造模特般的大长腿。无袖的款式更显得身姿苗条轻盈，浑身散发着柔情与职场女性的独特魅力。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>类型#上衣*图案#字母*图案#文字*图案#印花*衣样式#卫衣*衣门襟#套头*衣款式#拼接*衣款式#抽绳*衣款式#连帽*衣款式#罗纹</td>\n",
       "      <td>这款套头卫衣共有帅气黑和活力红两色可入，满足菇凉们不同风格的搭配需求。采用经典耐看的抽绳连帽设计，上身自带减龄属性。在胸前以趣味满满的半露字母印花点缀，尽显与众不同的高街时髦范。而以紧实罗纹拼接袖口及衣摆，则更能起到巧妙修饰身形的作用。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>类型#上衣*颜色#紫色*颜色#银色*风格#休闲*图案#刺绣*衣样式#卫衣*衣款式#连帽</td>\n",
       "      <td>紫色总是自带高傲和神秘的气息，拥有着让人着迷的魅力。moco的这款卫衣，采用深紫色的衣身，更具张扬个性。而银色的赛车风标志刺绣，则减弱了整体的刺眼效果，并带来潮范个性。还有连帽的设计，展现出休闲风的舒适感和轻松味道。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"], num_examples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb27556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb608702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(example, tokenizer, ignore_label_id=-100):\n",
    "    question = prompt_text + example['content']\n",
    "    if example.get('input', None) and example['input'].strip():\n",
    "        question += f'\\n{example[\"input\"]}'\n",
    "\n",
    "    answer = example['summary']\n",
    "\n",
    "    q_ids = tokenizer.encode(text=question, add_special_tokens=False)\n",
    "    a_ids = tokenizer.encode(text=answer, add_special_tokens=False)\n",
    "\n",
    "    if len(q_ids) > max_input_length - 2:  \n",
    "        q_ids = q_ids[:max_input_length - 2]\n",
    "    if len(a_ids) > max_output_length - 1:  \n",
    "        a_ids = a_ids[:max_output_length - 1]\n",
    "\n",
    "    input_ids = tokenizer.build_inputs_with_special_tokens(q_ids, a_ids)\n",
    "    question_length = len(q_ids) + 2  \n",
    "\n",
    "    labels = [ignore_label_id] * question_length + input_ids[question_length:]\n",
    "    return {'input_ids': input_ids, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36babeb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64790, 64792, 30910, 33467, 31010, 49534, 30998, 33692, 31010, 34198, 30998, 32799, 31010, 37785, 30998, 32799, 31010, 40589, 30998, 55500, 46025, 31010, 54988, 55647, 30998, 55500, 54811, 58709, 31010, 55097, 55759, 30998, 55500, 54811, 58709, 31010, 54712, 54882, 30998, 55500, 40877, 31010, 55529, 55379, 30998, 55500, 40877, 31010, 57551, 55889, 30998, 55500, 40877, 31010, 55097, 55759, 30998, 55500, 40877, 31010, 56897, 54882, 30910, 42733, 34198, 44109, 56778, 35191, 37785, 54542, 35614, 31123, 54733, 40284, 56597, 55857, 31123, 51827, 54963, 55400, 31155, 54589, 56278, 55090, 54888, 31123, 32286, 56310, 56128, 54715, 56409, 31155, 47987, 31735, 32177, 54536, 54575, 54660, 31123, 36039, ...]</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 42733, 34198, 44109, 56778, 35191, 37785, 54542, 35614, 31123, 54733, 40284, 56597, 55857, 31123, 51827, 54963, 55400, 31155, 54589, 56278, 55090, 54888, 31123, 32286, 56310, 56128, 54715, 56409, 31155, 47987, 31735, 32177, 54536, 54575, 54660, 31123, 36039, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "column_names = dataset['train'].column_names\n",
    "tokenized_dataset = dataset['train'].map(\n",
    "    lambda example: tokenize_func(example, tokenizer),\n",
    "    batched=False, \n",
    "    remove_columns=column_names\n",
    ")\n",
    "show_random_elements(tokenized_dataset, num_examples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd2a4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.shuffle(seed=seed)\n",
    "tokenized_dataset = tokenized_dataset.flatten_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0eb37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class DataCollatorForChatGLM:\n",
    "    def __init__(self, pad_token_id: int, max_length: int = 2048, ignore_label_id: int = -100):\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.ignore_label_id = ignore_label_id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch_data: List[Dict[str, List]]) -> Dict[str, torch.Tensor]:\n",
    "        len_list = [len(d['input_ids']) for d in batch_data]\n",
    "        batch_max_len = max(len_list) \n",
    "\n",
    "        input_ids, labels = [], []\n",
    "        for len_of_d, d in sorted(zip(len_list, batch_data), key=lambda x: -x[0]):\n",
    "            pad_len = batch_max_len - len_of_d \n",
    "            ids = d['input_ids'] + [self.pad_token_id] * pad_len\n",
    "            label = d['labels'] + [self.ignore_label_id] * pad_len\n",
    "            if batch_max_len > self.max_length:\n",
    "                ids = ids[:self.max_length]\n",
    "                label = label[:self.max_length]\n",
    "            input_ids.append(torch.LongTensor(ids))\n",
    "            labels.append(torch.LongTensor(label))\n",
    "\n",
    "        input_ids = torch.stack(input_ids)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return {'input_ids': input_ids, 'labels': labels}\n",
    "\n",
    "data_collator = DataCollatorForChatGLM(pad_token_id=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7f2578b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4af6f39b8e94c4f8e37d5e7886d08df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, BitsAndBytesConfig\n",
    "\n",
    "q_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_quant_type='nf4', \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_id, quantization_config=q_config, device_map='auto', trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94d57292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3739.69MiB\n"
     ]
    }
   ],
   "source": [
    "memory_footprint_bytes = model.get_memory_footprint()\n",
    "memory_footprint_mib = memory_footprint_bytes / (1024 ** 2)\n",
    "print(f\"{memory_footprint_mib:.2f}MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01add0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "kbit_model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffad96d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query_key_value']\n"
     ]
    }
   ],
   "source": [
    "from peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
    "\n",
    "target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['chatglm']\n",
    "print(target_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb0398f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import TaskType, LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=target_modules,\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60e3b476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 974,848 || all params: 6,244,558,848 || trainable%: 0.0156\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "qlora_model = get_peft_model(kbit_model, lora_config)\n",
    "qlora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0add566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "save_path = './data/hf/models/chatglm3-6b-qlora'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,                              # 输出目录\n",
    "    per_device_train_batch_size=6,                     # 每个设备的训练批量大小\n",
    "    gradient_accumulation_steps=8,                    # 梯度累积步数\n",
    "    learning_rate=1e-3,                                # 学习率\n",
    "    num_train_epochs=1,                                # 训练轮数\n",
    "    lr_scheduler_type=\"linear\",                        # 学习率调度器类型\n",
    "    warmup_ratio=0.1,                                  # 预热比例\n",
    "    logging_steps=100,                                 # 日志记录步数\n",
    "    optim=\"adamw_torch\",                               # 优化器类型\n",
    "    fp16=True,                                         # 是否使用混合精度训练\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64dcb8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.241, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qlora_model, args=training_args, train_dataset=tokenized_dataset, data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db558efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='2387' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   9/2387 01:19 < 7:28:16, 0.09 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344069d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
