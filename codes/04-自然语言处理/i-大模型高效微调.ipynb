{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3418a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'95a8e8fdd41cdc4b\\r\\nx-scheme: https\\r\\nx-request-id: 70e4d268482fa4f027007ded9023f539\\r\\nx-real-ip: 111']\n",
      "Bad pipe message: %s [b'99.82.105\\r\\nx-original-forwarded-for: 111.199.', b'.105\\r\\nx-forwarded-scheme: https\\r\\nx-forwarded-proto: htt']\n",
      "Bad pipe message: %s [b',http\\r\\nx-forwarded-port: 443,80\\r\\nx-forwarded-host: 1fcab7d27eec46b895a8e8fdd41cdc4b--45085.ap-shanghai.cloudstu', b'o.club\\r\\nx-forwarded-for: 111.199.82.105, 172.16.5.13, 172.18.138.4, 172.17.83.15,::ffff:10.89.0.2\\r\\n', b'client-proto-ver: HTTP/2.0\\r\\nx-client-proto: https\\r\\nupgrade-insecure-requests: 1\\r\\nsec-fetch-storage-access: active\\r\\nsec-', b'tch-site: same-site\\r\\nsec-fetch-mode: navigate\\r\\nsec-fetch-dest: iframe\\r\\nsec-ch-ua-platform: \"macOS\"\\r\\ns', b'-ch-ua-mobile: ?0\\r\\nsec-ch-ua: \"Not;A=Brand\";v=\"99\", \"Google Chrome\";v=\"139\", \"Chromium\";v=\"139\"\\r\\nref', b'er: https://1fcab7d27eec46b895a8e8fdd41cdc4b.ap-shanghai.cloudstudio.club/\\r\\npriority: u=0, i\\r\\naccept']\n",
      "Bad pipe message: %s [b'anguage: zh-CN,zh;q=0.9,en;q=0.8\\r\\naccept-enc']\n",
      "Bad pipe message: %s [b'ing: gzip, deflate, br, zstd\\r\\naccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/w']\n",
      "Bad pipe message: %s [b'p,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\\r\\nuser-agent: Mozilla/5.0 (Macintosh; I']\n",
      "Bad pipe message: %s [b'el Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36\\r\\nhost: 1fcab7d27ee']\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea7fb9246c643298d5bcdd8bc5b11bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2b897fef144bdf8d5b8028ed1dfeec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2e0ac54c1e48bfa34bad58c9e8a2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000c5b661de3491c87a198e2631700bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5582afd35334451f98d913ce49e66698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5fc01f13654d3f8ee5b3f6d80e1f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabbcf21bd264582b4a5d02ab0137830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, OPTForCausalLM\n",
    "\n",
    "model_id = \"facebook/opt-6.7b\"\n",
    "\n",
    "model = OPTForCausalLM.from_pretrained(model_id, load_in_8bit=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9f024f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Python is the best programming language.\n",
      "I'm not sure if you're being sarcastic or not, but I'm going to assume you're being serious.                  \n"
     ]
    }
   ],
   "source": [
    "text = \"Python is the best programming language.\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)  \n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9269ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b7f45ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.80GB\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 4096, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 4096)\n",
      "      (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "            (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "            (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "            (out_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "          (fc2): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "memory_footprint_bytes = model.get_memory_footprint()\n",
    "memory_footprint_mib = memory_footprint_bytes / (1024 ** 3)  # 转换为 GB\n",
    "\n",
    "print(f\"{memory_footprint_mib:.2f}GB\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c3a4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,666,862,592 || trainable%: 0.1258\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): OPTForCausalLM(\n",
      "      (model): OPTModel(\n",
      "        (decoder): OPTDecoder(\n",
      "          (embed_tokens): Embedding(50272, 4096, padding_idx=1)\n",
      "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 4096)\n",
      "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "          (layers): ModuleList(\n",
      "            (0-31): 32 x OPTDecoderLayer(\n",
      "              (self_attn): OPTSdpaAttention(\n",
      "                (k_proj): lora.Linear8bitLt(\n",
      "                  (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (v_proj): lora.Linear8bitLt(\n",
      "                  (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_proj): lora.Linear8bitLt(\n",
      "                  (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_proj): lora.Linear8bitLt(\n",
      "                  (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "              (fc2): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "              (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=50272, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 创建一个LoraConfig对象，用于设置LoRA的配置参数\n",
    "config = LoraConfig(\n",
    "    r=8,            # LoRA的秩，影响LoRA矩阵的大小\n",
    "    lora_alpha=32,  # LoRA适应的比例因子\n",
    "    # 指定将LoRA应用到的模型模块，通常是attention和全连接层的投影\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc_in\", \"fc_out\"],\n",
    "    lora_dropout=0.05,     # 在LoRA模块中使用的dropout率\n",
    "    bias=\"none\",           # 设置bias的使用方式，这里没有使用bias\n",
    "    task_type=\"CAUSAL_LM\"  # 任务类型，这里设置为因果(自回归）语言模型\n",
    ")\n",
    "\n",
    "# 使用get_peft_model函数和给定的配置来获取一个PEFT模型\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff1eb109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e70dc2b1b640d1a20f18cebe0de8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d6c1f9319e4fcd80c29f0f4f7bb1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "quotes.jsonl:   0%|          | 0.00/647k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e82797aaa6641c78377b39fa735b805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['quote', 'author', 'tags'],\n",
      "    num_rows: 2508\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Abirate/english_quotes\")\n",
    "print(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f87e64af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>author</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“The good thing about science is that it's true whether or not you believe in it.”</td>\n",
       "      <td>Neil deGrasse Tyson</td>\n",
       "      <td>[belief, science, true]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Donâ€™t grieve. Anything you lose comes round in another form.”</td>\n",
       "      <td>Rumi</td>\n",
       "      <td>[bereavement, consolation, grief, loss, reincarnation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Religion has actually convinced people that there's an invisible man living in the sky who watches everything you do, every minute of every day. And the invisible man has a special list of ten things he does not want you to do. And if you do any of these ten things, he has a special place, full of fire and smoke and burning and torture and anguish, where he will send you to live and suffer and burn and choke and scream and cry forever and ever 'til the end of time! But He loves you. He loves you, and He needs money! He always needs money! He's all-powerful, all-perfect, all-knowing, and all-wise, somehow just can't handle money!”</td>\n",
       "      <td>George Carlin</td>\n",
       "      <td>[atheism, humor, life, religion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“Either write something worth reading or do something worth writing.”</td>\n",
       "      <td>Benjamin Franklin</td>\n",
       "      <td>[hmmm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Read, read, read. Read everything -- trash, classics, good and bad, and see how they do it. Just like a carpenter who works as an apprentice and studies the master. Read! You'll absorb it.Then write. If it's good, you'll find out. If it's not, throw it out of the window.”</td>\n",
       "      <td>William Faulkner</td>\n",
       "      <td>[reading, writing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>“The last enemy that shall be destroyed is death.”</td>\n",
       "      <td>J.K. Rowling,</td>\n",
       "      <td>[bible, death, enemy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>“I think that if I ever have kids, and they are upset, I won't tell them that people are starving in China or anything like that because it wouldn't change the fact that they were upset. And even if somebody else has it much worse, that doesn't really change the fact that you have what you have.”</td>\n",
       "      <td>Stephen Chbosky,</td>\n",
       "      <td>[honesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>“The past has no power over the present moment.”</td>\n",
       "      <td>Eckhart Tolle</td>\n",
       "      <td>[education, inspirational, life, philosophy, truth, wisdom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>“There are wounds that never show on the body that are deeper and more hurtful than anything that bleeds.”</td>\n",
       "      <td>Laurell K. Hamilton,</td>\n",
       "      <td>[depression, pain, trauma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>“Science and religion are not at odds. Science is simply too young to understand.”</td>\n",
       "      <td>Dan Brown,</td>\n",
       "      <td>[books, dan-brown, religion, science]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))\n",
    "    \n",
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63c979a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca42e162f3cc43e2828f514088ebb9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenized_dataset = dataset.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b78535fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "save_path = \"./data/hf/models/opt-6.7b-lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,           # 指定模型输出和保存的目录\n",
    "    per_device_train_batch_size=4,  # 每个设备上的训练批量大小\n",
    "    learning_rate=2e-4,             # 学习率\n",
    "    fp16=True,                      # 启用混合精度训练，可以提高训练速度，同时减少内存使用\n",
    "    logging_steps=20,               # 指定日志记录的步长，用于跟踪训练进度\n",
    "    max_steps=100,                  # 最大训练步长\n",
    "    num_train_epochs=1              # 训练的总轮数\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a32d31b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.241, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                               # 指定训练时使用的模型\n",
    "    train_dataset=tokenized_dataset[\"train\"],  # 指定训练数据集\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "model.use_cache = False # 禁用模型的自回归生成缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b2876c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:56, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.808700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.800100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.954100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.8899650192260742, metrics={'train_runtime': 177.8259, 'train_samples_per_second': 2.249, 'train_steps_per_second': 0.562, 'total_flos': 1364172665978880.0, 'train_loss': 1.8899650192260742, 'epoch': 0.1594896331738437})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04726aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1889d6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is the best programming language.\n",
      ",,,,,, The The The The The.. I I am in in in in in the The The The The I am am a a a Son Son,,,....\n"
     ]
    }
   ],
   "source": [
    "lora_model = trainer.model\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "out = lora_model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879775c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
