# 学习的相关技巧

## 更多参数的网络

使用sklearn的手写数字训练神经网络

### 加载数据集

```python
from sklearn import datasets
from sklearn.preprocessing import StandardScaler

digits = datasets.load_digits()

scaler = StandardScaler()
scaler.fit(digits.data)
x = scaler.transform(digits.data)
y = digits.target

print(y[:1])
print(x[:1])
```

`scaler`归一化到0~1之间，64维。划分训练集和测试集，并将数据转换为`tesnor`

```python
import torch
from sklearn.model_selection import train_test_split

train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=42)
train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=360, random_state=42)

train_x = torch.FloatTensor(train_x)
train_y = torch.LongTensor(train_y)
val_x = torch.FloatTensor(val_x)
val_y = torch.LongTensor(val_y)
test_x = torch.FloatTensor(test_x)
test_y = torch.LongTensor(test_y)

print(train_x.size())
print(train_y.size())
print(val_x.size())
print(val_y.size())
print(test_x.size())
print(test_y.size())
```

将数据包装成`DataLoader`形式才可以进行训练

```python
from torch.utils.data import TensorDataset, DataLoader

train_dataset = TensorDataset(train_x, train_y)
val_dataset = TensorDataset(val_x, val_y)
test_dataset = TensorDataset(test_x, test_y)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)
test_loader = DataLoader(test_dataset, batch_size=32)
```

* `TensorDataset`是使训练数据集包含，特征和标签的组合。
* `DataLoader`是训练数据的读取器，可以控制每次读取数据的数量。

### 构建网络

```python
from torch import nn
from torchinfo import summary

model = nn.Sequential(
    nn.Linear(64, 100),
    nn.Sigmoid(),
    nn.Linear(100, 50),
    nn.Sigmoid(),
    nn.Linear(50, 10),
)

print(summary(model, input_size=(1, 64)))
```

上述神经网络结构如下

```mermaid
flowchart LR
		a(输入层 X-64)-->b(隐层 100)-->c(隐层 50)-->d(输出层 Y-10)
```

使用正态分布对模型参数进行初始化

```python
def uniform_init_parameters(m):
    if isinstance(m, nn.Linear):
        torch.manual_seed(42)
        nn.init.uniform_(m.weight)
        nn.init.zeros_(m.bias)

model.apply(uniform_init_parameters)
```

### 损失函数

定义交叉熵损失函数和优化器

```python
from torch import optim

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=1)
```

### 模型训练

将训练过程封装为函数

```python
import torch

def fit_model(model, loss_fn, optimizer, epochs=100):
    train_losses = []
    train_accuracies = []
    test_losses = []
    test_accuracies = []
    step1 = epochs / 10

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for images, labels in train_loader:
            # 前向传播
            outputs = model(images)
            loss = loss_fn(outputs, labels)

            # 反向传播和优化
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # 统计
            running_loss += loss.item() # loss.item()返回的是一个标量，是一个batch的平均loss
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        # 计算训练指标
        train_loss = running_loss / len(train_loader) # len(train_loader)返回的是batch的个数
        train_accuracy = 100 * correct / total

        train_losses.append(train_loss)
        train_accuracies.append(train_accuracy)

        # 验证阶段
        model.eval()
        test_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for images, labels in val_loader:
                outputs = model(images)
                loss = loss_fn(outputs, labels)

                test_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1) # torch.max返回的是最大值和最大值的索引
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        # 计算验证指标
        test_loss = test_loss / len(val_loader)
        test_accuracy = 100 * correct / total
        test_losses.append(test_loss)
        test_accuracies.append(test_accuracy)


        if (epoch + 1) % step1 == 0:
            print(f"Epoch {epoch+1} / {epochs}: "
                  f"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, "
                  f"Val Loss: {test_loss:.4f}, Val Acc: {test_accuracy:.2f}%")

    # 打印最终结果
    print("\nTraining complete!")
    print(f"Best Test Accuracy: {max(test_accuracies):.2f}% at epoch {test_accuracies.index(max(test_accuracies))+1}")

    return {
        'train_losses': train_losses,
        'train_accuracies': train_accuracies,
        'val_losses': test_losses,
        'val_accuracies': test_accuracies
    }
```

训练模型

```python
result = fit_model(model, loss_fn, optimizer, epochs=200)
```

绘制损失曲线和准确率曲线

```python
import matplotlib.pyplot as plt

def plot_results(result):
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(result['train_accuracies'], label='Train Accuracy', linewidth=3)
    plt.plot(result['val_accuracies'], label='Val Accuracy', linewidth=3)
    plt.title('Accuracy vs. Epochs', fontsize=16)
    plt.xlabel('Epoch', fontsize=16)
    plt.ylabel('Accuracy (%)', fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(result['train_losses'], label='Train Loss', linewidth=3)
    plt.plot(result['val_losses'], label='Val Loss', linewidth=3)
    plt.title('Loss vs. Epochs', fontsize=16)
    plt.xlabel('Epoch', fontsize=16)
    plt.ylabel('Loss', fontsize=16)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

plot_results(result)
```

### 模型评估

将训练代码的测试过程封装成一个函数

```python
def evaluate_model(model, test_loader):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in test_loader:
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)  # 获取预测结果
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    test_accuracy = 100 * correct / total
    print(f"Test Accuracy: {test_accuracy:.2f}%")
    return test_accuracy

test_accuracy = evaluate_model(model, test_loader)
```

## 参数的更新

### 随机梯度下降

从训练数据中随机选择数据进行梯度下降称为随机梯度下降算法（stochastic gradient descent），其权重的更新公式如下
$$
W \leftarrow W-\eta\frac{\partial L}{\partial W}
$$

其中$\eta$表示下降的学习率。SDG优化器的缺点，对于函数

$$
f(x, y)=\frac{1}{20}x^2+y^2 \tag{1}
$$
其图像如为

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nn/Xnip2025-01-20_11-59-05.jpg" style="zoom:40%;" />

而其梯度方向图为

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nn/Xnip2025-01-20_12-02-25.jpg" style="zoom:35%;" />

 使用SGD对函数 $(1)$ 优化路径如下，路径图如下

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nn/Xnip2025-01-20_13-38-13.jpg" style="zoom:35%;" />

SGD的缺点是，如果函数的形状非均向搜索的路径就会非常低效。上面例子的训练过程使用的就是SGD优化器。

### Momentum

动量优化（Momentum）器的数学公式如下
$$
v\leftarrow \alpha v-\eta\frac{\partial L}{\partial W} \\
W\leftarrow W+ v
$$

* $\alpha$设定为逐渐减速的任务。
* 初始时$v=0$。
* 之后每次下降调用上一次的$v$值。
* 计算梯度的指数加权平均数，并利用该值来更新参数值。

使用Momentum对函数 $(1)$ 优化路径如下

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nn/Xnip2025-01-20_13-56-39.jpg" style="zoom:35%;" />

```python
model.apply(uniform_init_parameters)
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
result = fit_model(model, loss_fn, optimizer, epochs=200)
plot_results(result)
```

### AdaGrad

在神经网络的学习中，学习率$\eta$的值很重要：

* 学习率过小，会导致学习花费过多时间；
* 反过来，学习率过大，则会导致学习发散而不能正确进行。

在关于学习率的有效技巧中，有一种被称为学习率衰减的方法，即随着学习的进行，使学习率逐渐减小。AdaGrad会为参数的每个元素适当地调整学习率，数学式表示为
$$
h \leftarrow h+\frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W} \\
W \leftarrow W - \eta \frac{1}{\sqrt{h}}\frac{\partial L}{\partial W}
$$

* AdaGrad是对学习率进行修正。
* 按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小。
* 学习越深入，更新的幅度就越小。如果无止境地学习，更新量就会变为0。

使用AdaGrad对函数 $(1)$ 优化路径如下

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nn/Xnip2025-01-21_08-48-10.jpg" style="zoom:35%;" />

```python
model.apply(uniform_init_parameters)
optimizer = optim.Adagrad(model.parameters(), lr=0.05)
result = fit_model(model, loss_fn, optimizer, epochs=200)
plot_results(result)
```

### Adam

直观地讲，Adam就是融合了Momentum 和 AdaGrad的方法。Adam会设置3个超参数：

1. 学习率`lr`，超参数用户选择。
2. 一次momentum系数`beta1`，通常设置为0.9
3. 二次momentum系数`beta2`，通常设置为0.999

使用Adam对函数 $(1)$ 优化路径如下

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nn/Xnip2025-01-21_08-59-04.jpg" style="zoom:35%;" />

```python
model.apply(uniform_init_parameters)
optimizer = optim.Adam(model.parameters(), lr=0.01)
result = fit_model(model, loss_fn, optimizer, epochs=100)
plot_results(result)
```

## 权重的初始值

在神经网络的学习中，权重的初始值特别重要，经常关系到神经网络的学习能否成功。

> [!Note]
>
> 可以将权重初始值设为0吗？

将权重初始值设为0的话，将无法正确进行学习。

> [!attention]
>
> 严格地说，不能将权重初始值设成一样的值。

1. 为了防止“权重均一化”，必须随机生成初始值。

在误差反向传播法中，所有的权重值都会进行相同的更新。比如，在2层神经网络中，假设第1层和第2层的权重为0。这样一来，正向传播时，因为输入层的权重为0，所以第2层的神经元全部会被传递相同的值。第2层的神经元中全部输入相同的值，这意味着反向传播时第2层的权重全部都会进行相同的更新。因此，权重被更新为相同的值，这使得神经网络拥有许多不同的权重的意义丧失了。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nn/Xnip2025-01-16_13-50-37.jpg" style="zoom:35%;" />

2. 在逻辑回归中为了抑制权重的过拟合，使用了正则项，正则项的目的就是抑制权重过大，所以权重应该是一个很小的值。

> [!warning]
>
> 在权重初始化时，初始权重应该是一个较小的随机值。

实践中使用`0.01 * np.random.randn(10, 100)`初始化权重值，由高斯分布生成的值乘以0.01后得到的值（标准差为0.01的高斯分布）。

### sigmoid的初始值

sigmoid函数是S型函数，各层的激活值呈偏向0和1的分布。

*  偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失。

* 推荐使用Xavier初始值：如果前一层的节点数为$n$，则初始值使用标准差为$\frac{1}{\sqrt{n}}$的分布。该方法的基本思想是，各层的激活值和梯度的方差在传播过程中保持一致。

```python
def xavier_init_parameters(m):
    if isinstance(m, nn.Linear):
        torch.manual_seed(42)
        nn.init.xavier_uniform_(m.weight)
        nn.init.zeros_(m.bias)

model.apply(xavier_init_parameters)
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
result = fit_model(model, loss_fn, optimizer, epochs=100)
plot_results(result)
```

### ReLU的权重初始值

当激活函数使用ReLU时，一般使用He初始值。使用标准差为$\sqrt{\frac{2}{n}}$的高斯分布。因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数。

```python
model2 = nn.Sequential(
    nn.Linear(64, 100),
    nn.ReLU(),
    nn.Linear(100, 50),
    nn.ReLU(),
    nn.Linear(50, 10),
)

def he_init_parameters(m):
    if isinstance(m, nn.Linear):
        torch.manual_seed(42)
        nn.init.kaiming_uniform_(m.weight)
        nn.init.zeros_(m.bias)

model2.apply(he_init_parameters)
optimizer = optim.SGD(model2.parameters(), lr=0.001, momentum=0.9)
result = fit_model(model2, loss_fn, optimizer, epochs=200)
plot_results(result)
```

当初始值为He初始值时，各层中分布的广度相同。由于即便层加深，数据的广度也能保持不变，因此逆向传播时，也会传递合适的值。

> [!warning]
>
> 神经网络学习出来的参数就是一系列非常小的数值，但又不是0。

## 正则化

### Batch Normalization

Batch Norm就是以进行学习时的mini-batch为单位，按mini-batch进行归一化。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nn/9b7f715ee6ce4769af5f48336529e450.png" style="zoom:60%;" />

其中数据集$B=\{x_1, x_2,…,x_m\}$表示mini-batch中的全部数据，上面的相当于对mini-batch的数据进行均值方差归一化。Batch Norm层会对正规化后的数据进行缩放和平移的变换，公式为
$$
f(x)=\gamma\cdot\frac{x-\text{E}(x)}{\sqrt{\text{Var}(x)}+\epsilon}+\beta
$$
1. $\gamma$和$\beta$是可学习的参数，它相当于对标准化后的值做了一个线性变换，$\gamma$为系数，$\beta$为偏置。
2. $\epsilon$通常为1e-5，避免分母为0。
3. $\text{E}(x)$表示变量的均值。
4. $\text{Var}(x)$表示变量的方差。

Batch Norm方法的优点：可以增大学习速率；不那么依赖初始值；抑制过拟合。

> [!warning]
>
> 建议BN层被插入在ReLU激活层前面

```python
model3 = nn.Sequential(
    nn.Linear(64, 100),
    nn.BatchNorm1d(100),
    nn.Sigmoid(),
    nn.Linear(100, 50),
    nn.BatchNorm1d(50),
    nn.Sigmoid(),
    nn.Linear(50, 10),
)

model3.apply(xavier_init_parameters)
optimizer = optim.SGD(model3.parameters(), lr=0.1, momentum=0.9)
result = fit_model(model3, loss_fn, optimizer, epochs=100)
plot_results(result)
```

> [!warning]
>
> PyTroch可以实现，部分参数正则化或不同参数正则化不同值。

### L2正则

正则项可以控制权重的无限增大，可以用于抑制过拟合。在深度学习中同样有效。优化器直接内置L2正则化（模型参数添加正则化）。 

```python
model.apply(xavier_init_parameters)
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.01)
result = fit_model(model, loss_fn, optimizer, epochs=100)
plot_results(result)
```

`weight_decay`值越高，L2正则的强度越大。

### Dropout

Dropout 是一种在学习的过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递，示意图如下

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nn/Xnip2025-01-24_20-29-40.jpg" style="zoom:35%;" />

训练时，每传递一次数据，就会随机选择要删除的神经元；测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出。

可以将Dropout理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。这可以看成机器学习中的集成方法（ensemble technique）。集成模型一般优于单一模型，因为它们可以捕获更多的随机性。

推理时，通过对神经元的输出乘以删除比例，取得模型的平均值。最终，Dropout将集成学习的效果通过一个网络实现了。

```python
model4 = nn.Sequential(
    nn.Linear(64, 100),
    nn.Sigmoid(),
    nn.Dropout(p=0.5),
    nn.Linear(100, 50),
    nn.Sigmoid(),
    nn.Dropout(p=0.5),
    nn.Linear(50, 10),
)

model4.apply(xavier_init_parameters)
optimizer = optim.SGD(model4.parameters(), lr=0.1, momentum=0.9)
result = fit_model(model4, loss_fn, optimizer, epochs=100)
plot_results(result)
```

## 超参数的选择

超参数的值设置合适，模型的性能就会很好，且训练速度快，但在决定超参数的过程中一般会伴随很多的试错。进行超参数的最优化时，逐渐缩小“合适”超参数的范围。优化步骤如下：

1. 设定超参数的范围（大致设定一个范围）。
2. 从设定的超参数范围中随机采样。
3. 使用步骤2中采样到的超参数的值进行学习，通过验证数据评估识别精度（但是要将epoch设置得很小）。
4. 重复步骤2和步骤3（ 100 次等），根据它们的识别精度的结果，缩小超参数的范围。

反复进行上述操作，不断缩小超参数的范围，在缩小到一定程度时，从该范围中选出一个超参数的值。这就是进行超参数的最优化的一种方法。

> [!attention]
>
> 不能使用测试数据评估超参数的性能。

根据上面的训练过程，选择合适参数

```python
model_best = nn.Sequential(
    nn.Linear(64, 100),
    nn.BatchNorm1d(100),
    nn.ReLU(),
    nn.Dropout(p=0.2),  
    nn.Linear(100, 50),
    nn.BatchNorm1d(50),
    nn.ReLU(),
    nn.Dropout(p=0.2), 
    nn.Linear(50, 10),
)

model_best.apply(he_init_parameters)
optimizer = optim.Adam(model_best.parameters(), lr=0.001)
result = fit_model(model_best, loss_fn, optimizer, epochs=100)
plot_results(result)
```

测试模型性能

```python
evaluate_model(model_best, test_loader)
```

打印模型的混淆矩阵

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

def evaluate_model_with_confusion_matrix(model, test_loader):
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=range(10), 
                yticklabels=range(10))
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()
    
    return cm

cm = evaluate_model_with_confusion_matrix(model_best, test_loader)
print("Confusion Matrix:")
print(cm)
```

统计识别错误的数据

```python
def evaluate_with_errors(model, test_loader):
    model.eval()
    error_samples = []  # 存储所有错误样本的列表
    
    with torch.no_grad():
        for batch_idx, (inputs, labels) in enumerate(test_loader):
            outputs = model(inputs)
            outputs = torch.softmax(outputs, dim=1) # 计算每个类别的概率
            _, preds = torch.max(outputs, 1)
            
            wrong_indices = (preds != labels).nonzero(as_tuple=True)[0]
            for idx in wrong_indices:
                error_sample = {       
                    'features': inputs[idx].cpu().numpy().copy(),  
                    'true_label': labels[idx].item(),             
                    'pred_label': preds[idx].item(),             
                    'probabilities': outputs[idx].cpu().numpy().copy()
                }
                error_samples.append(error_sample)    
    return error_samples

error_data = evaluate_with_errors(model_best, test_loader)
print(f"\n总错误样本数: {len(error_data)}")
```

`outputs = torch.softmax(outputs, dim=1)`计算每个类别的概率，否则输出不是概率形式。打印错误样本图像

```python
def plot_error_samples(error_data):
    num_samples = len(error_data)
    if num_samples == 0:
        print("No error samples to plot.")
        return
    
    max_cols = 3
    rows = (num_samples + max_cols - 1) // max_cols
    cols = min(num_samples, max_cols)
    
    fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*3))
    if rows == 1:
        axes = axes.reshape(1, -1)
    elif cols == 1:
        axes = axes.reshape(-1, 1)
    for i in range(num_samples):
        row = i // max_cols
        col = i % max_cols
        ax = axes[row, col]
        
        sample = error_data[i]
        ax.imshow(sample['features'].reshape(8, 8), cmap='gray')
        ax.set_title(f"True: {sample['true_label']} Pred: {sample['pred_label']} \n Prob: {sample['probabilities'].max():.4f}" ,
                     fontsize=14)
        ax.axis('off')

    for i in range(num_samples, rows * cols):
        row = i // max_cols
        col = i % max_cols
        fig.delaxes(axes[row, col])
    
    plt.tight_layout()
    plt.show()

plot_error_samples(error_data)
```



