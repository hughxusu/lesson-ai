# 深度学习

深度学习就是神经网络加深了网络层。基于之前介绍的网络，只需通过叠加层，就可以创建深度网络。

2006年，杰弗里-辛顿以正式提出了深度学习的概念。2012年，在著名的ImageNet图像识别大赛中，杰弗里-辛顿领导的小组采用深度学习模型AlexNet一举夺冠。

深度学习只是机器学习的一种工具。

> [!warning]
>
> 只有感性认识，理论支持较弱。

深度学习就是寻找一个合适的网络组合，达到非常好的分类效果。

> [!warning]
>
> 层数越多模型越复杂，容易过拟合。在满足任务的条件下层数越少越好。

## 加深网络

卷积神经网络的发展，基于ImageNet数据评测

| 网络名称   | 发表时间 | 网络层数 | 参数数量 | Top-1 / Top-5 错误率 |
| :--------- | :------- | :------- | :------- | :------------------- |
| AlexNet    | 2012     | 8 层     | ~60M     | 37.5% / 17.0%        |
| VGG-16     | 2014     | 16 层    | ~138M    | 28.5% / 9.9%         |
| VGG-19     | 2014     | 19 层    | ~144M    | 28.7% / 9.8%         |
| GoogLeNet  | 2014     | 22 层    | ~7M      | 29.9% / 10.1%        |
| ResNet-34  | 2015     | 34 层    | ~21.8M   | 24.7% / 7.8%         |
| ResNet-50  | 2015     | 50 层    | ~25.6M   | 22.9% / 6.7%         |
| ResNet-101 | 2015     | 101 层   | ~44.5M   | 21.8% / 6.1%         |
| ResNet-152 | 2015     | 152 层   | ~60.2M   | 21.4% / 5.7%         |

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nn/20190716171704843.png" style="zoom:80%;" />

### 加深网络的意义

[图像分类的算法比较](https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html) 从总体的趋势看，随着网络深度的增加，网络的性能不断提升。但加深层的重要性，现状是理论研究还不够透彻，相关理论还比较贫乏。可以从定性分析中大概阐述加深网络的好处。

1. 叠加小型滤波器来加深网络的好处是可以减少参数的数量，扩大感受野。
2. 通过叠加层，将 ReLU 等激活函数夹在卷积层的中间，进一步提高了网络的表现力。通过非线性函数的叠加，可以表现更加复杂的东西。
3. 通过加深网络，就可以分层次地分解需要学习的问题。可以将各层要学习的问题分解成容易解决的简单问题，从而可以进行高效的学习。在前面的卷积层中，神经元会对边缘等简单的形状有响应，随着层的加深，开始对纹理、物体部件等更加复杂的东西有响应。

### 网络的深度与广度

神经网络设计的两个方向

1. 设计更多的隐层。
2. 每一层设计更多的神经元。理论上可以证明当神经元N足够多的时候，使用sigmoid函数激活，一层的神经元可以拟合任何函数。

深度学习中单纯增加网络深度，会出现梯度消失和网络退化问题。但随着新的网结构推出，这两个问题在一定程度上得到解决。

> [!warning]
>
> 从实践结果来看，加深网络结构比加宽网络结构更有效。

### 拓展数据集

加深网络结构后，需要更多的数据进行训练。

> [!note]
>
> 如何在数据集的图像数量有限时，拓展数据集？

基于算法扩充训练图像。具体地说，对于输入图像，通过施加旋转、垂直或水平方向上的移动等微小变化，增加图像的数量。也可以通过裁剪、翻转、亮度等外观上变化、放大缩小尺度变化等。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nn/Xnip2025-01-28_19-11-42.jpg" style="zoom:35%;" />

## 学习的高速化

1. 基于GPU的高速化。深度学习中需要进行大量的乘积累加运算，使用GPU进行深度学习的运算可以加速训练过程。
2. 分布式学习。可以考虑在多个GPU或者多台机器上进行分布式计算。
3. 运算精度的位数缩减。计算机中为了表示实数，主要使用64位或者32位的浮点数。但在深度学习中并不那么高的精度，即便是16位的半精度浮点数（half float），也可以顺利地进行学习。

## 神经网络的优缺点

### 优点

* 精度高，性能优于传统的机器学习方法，甚至在某些领域超过了人类。
* 可以近似任意的非线性函数。
* 端到端的学习，省略了特征提取的过程。
* 随着计算发展，有大量框架和库可以调用。

### 缺点

* 黑箱，很难解释模型是怎么工作的。
* 需要大量数据。
* 训练时间长，需要大量算力。
* 网络结构复杂，需要调整超参数。
* 小数据集上表现不佳，容易发生过拟合。

## 机器学习框架

| 框架                                               | 优点                                                         | 缺点                                                         | 公司     |
| -------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | -------- |
| [Caffe](https://caffe.berkeleyvision.org/)         | \- 简单易用的接口                                            | \- 功能相对较为有限                                          | Facebook |
| [TensorFlow](https://www.tensorflow.org/?hl=zh-cn) | - 强大的生态系统和支持<br>- 良好的文档和社区支持<br>- 支持灵活的部署（包括移动端和嵌入式设备） | - 相对较复杂，学习曲线较陡峭<br>- 部分功能可能不够直观       | Google   |
| [PyTorch](https://pytorch.org/)                    | - 动态图模式更直观，易于调试<br>- 灵活性高，易于定制<br>- 易于在GPU上进行加速计算 | - 相对TensorFlow较新，生态系统可能不及其成熟<br>- 文档相对不够完善 | Facebook |
| [Keras](https://keras.io/)                         | - 高度模块化，易于使用<br>- 抽象层次较高，适合快速原型设计<br>- 与多个后端兼容（如TensorFlow、Theano等） | - 灵活性相对较差，不够适合定制<br>- 性能可能略逊于TensorFlow和PyTorch | Google   |
| [PaddlePaddle](https://www.paddlepaddle.org.cn/)   | - 易用性高，提供了易于上手的高级API和简单直观的编程接口<br>- 灵活性高，支持静态图和动态图两种模式 | - 生态系统相对较小，社区资源和第三方工具相对较少<br>- 文档和教程相对不足<br>- 国际化程度有限 | Baidu    |

