# YOLO系列算法

[YOLO](https://docs.ultralytics.com/zh#where-to-start)系列算法是一类典型的one-stage目标检测算法，其利用anchor box将分类与目标定位的回归问题结合起来，从而做到了高效、灵活和泛化性能好，所以在工业界也十分受欢迎。

| **版本**    | **发布时间** | **主要特点**                                             | **作者/团队**               | **框架** |
| ----------- | ------------ | -------------------------------------------------------- | --------------------------- | -------- |
| **YOLOv1**  | 2016年5月    | 首个单阶段检测框架，实时性强但定位精度较低。             | Joseph Redmon               | Darknet  |
| **YOLOv2**  | 2017年12月   | 引入批量归一化、锚框和多尺度训练，支持检测9000类物体。   | Joseph Redmon               | Darknet  |
| **YOLOv3**  | 2018年4月    | 采用Darknet-53骨干网络，引入多尺度预测，提升小物体检测。 | Joseph Redmon & Ali Farhadi | Darknet  |
| **YOLOv4**  | 2020年4月    | 结合数据增强、自适应锚点等技术，平衡速度与精度。         | Alexey Bochkovskiy          | Darknet  |
| **YOLOv5**  | 2020年6月    | 非官方版本，优化易用性和性能，支持PyTorch部署。          | Glenn Jocher                | PyTorch  |
| **YOLOv6**  | 2022年6月    | 美团团队开发，采用无锚点检测器，优化工业场景。           | 美团技术团队                | -        |
| **YOLOv7**  | 2022年7月    | 改进模型结构，提升硬件效率（如GPU利用率）。              | Alexey Bochkovskiy          | -        |
| **YOLOv8**  | 2023年1月    | 引入Anchor-free检测头和新骨干网络，支持图像分割。        | Ultralytics公司             | PyTorch  |
| **YOLOv9**  | 2024年2月    | 提出GELAN架构和可编程梯度信息（PGI），显著提升精度。     | Chien-Yao Wang等            | -        |
| **YOLOv10** | 2024年5月    | 清华大学团队解决NMS和计算冗余问题，兼顾速度与精度。      | 清华大学团队                | -        |
| **YOLOv11** | 2024年9月    | Ultralytics公司优化推理时间，适合低算力设备。            | Ultralytics公司             | PyTorch  |

* 早期版本（v1-v4）基于Darknet框架开发，从v5开始转向PyTorch，提升易用性和部署灵活性。
* 官方主线版本：v1-v4、v7-v9

## YOLO算法

Yolo算法采用统一的网络实现端到端的目标检测，以整张图像作为网络的输入，在输出目标位置的同时，输出目标类别，系统流程如下：

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/75d82659925934a9c8f3cea12db409fd.png" style="zoom:70%;" />

1. 将输入图片resize到$448\times448$。
2. 同时得到分类结果和目标位置，其速度相比R-CNN算法更快。

相关论文：[You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/pdf/1506.02640)

### 基本思想

Yolo（意思是You Only Look Once）算法，创造性的将候选区和目标分类合二为一，看一眼图片就能知道有哪些对象以及它们的位置。

Yolo模型，将原始图像划分为$7\times7=49$个网格（grid），每个网格允许预测2个包含某个对象的矩形框，即边框（bounding box），总共$49\times2=98$个。这98个预测区，很粗略的覆盖了整张图片，在这98个预测区中进行目标检测，得到98个区域的目标分类和回归结果，再进行NMS，得到最终结果。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/0370cc6078af89ce708f2057f01da44c.png" style="zoom:45%;" />

### 网络结构

YOLO的结构就是单纯的卷积、池化最后加了两层全连接，与前面介绍的CNN分类网络没有本质的区别。最大的差异是输出层用线性函数做激活函数，需要预测目标的位置和物体的类别。YOLO的整个结构，就是输入图片经过神经网络，变换得到一个输出的张量。
<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/e4b941926c583fcd32392abd9877093f.png" style="zoom:75%;" />

#### 网络输入

输入图像的大小固定为$448\times448$

#### 网络输出

网络的输出就是一个$7\times7\times30$ 的张量。根据YOLO的设计，输入图像被划分为$7\times 7$的网格（grid），输入图像中的每个网格对应输出一个30维的向量。
<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/a9e5ef7f3e43202d9183b6dcd9a78afc.png" style="zoom:55%;" />

30维的向量包含：

1. 2个bbox的位置和置信度
   1. bbox需要4个数值来表示其位置`(Center_x,Center_y,width,height)`，2个bbox共需要8个数值来表示其位置。
   2. 2个bbox的置信度
      * $\text{Pr}(\text{Object})$bbox是否包含物体的中心点。
      * $\text{IOU}^{\text{truth}}_{\text{pred}}$bbox比标注框的交并比。

$$
\text{Confidence}=\text{Pr}(\text{Object})\times\text{IOU}^{\text{truth}}_{\text{pred}}
$$

2. 网格属于20个类别的概率。YOLO V1基于VOC数据进行训练，支持20种对象分类。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/4a389e4a258eab8caf06f6a00cdc1b18.png" style="zoom:60%;" />

### 模型标注

训练图像如下

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/50fb4e68e0b5b403b5298ce112fbbd9f.png" style="zoom:85%;" />

输出结果标注如下

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/5e2113b15c94befe668f978e964382ed.png" style="zoom:55%;" />

1. 20个对象分类的概率。比如上图中自行车，中心点位置网格负责预测自行车，自行车的概率是1，其它概率为0。所有其它网格的30维向量中，自行车的概率都是0。
2. 2个bbox的置信度
   1. 负责检测目标的网格（自行车）
      1. 与标注值交并比大的框，置信度为1。
      2. 与标注值交并比小的框，置信度为0。
   2. 不负责检测目标的网格，置信度为0。
3. bbox的自信值为1的边框设置为标注值，为随机值。

### 损失函数

网络实际输出值如下

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/173264513821fd83c773ca8d82cae3a9.png" style="zoom:55%;" />

损失函数为样本标签和网络输出之间的偏差
$$
\begin{align}
\text{Loss} 
&= 
\lambda_{\text{coord}}\sum_{i=0}^{S^2}\sum_{j=0}^B
\mathbb{1}^{obj}_{ij}\left[(x_i-\hat{x}_i)^2-(y_i-\hat{y}_i)^2\right] & \text{边框中心点误差} \\
&+
\lambda_{\text{coord}}\sum_{i=0}^{S^2}\sum_{j=0}^B
\mathbb{1}^{obj}_{ij}
\left[(\sqrt{w_i} -\sqrt{\hat{w}} _i)^2-(\sqrt{h_i} -\sqrt{\hat{h}_i} )^2\right] & \text{边框中宽、高误差} \\
&+
\sum_{i=0}^{S^2}\sum_{j=0}^B
\mathbb{1}^{obj}_{ij}(C_i-\hat{C}_i)^2 & \text{有对象置信度误差} \\ 
&+
\lambda_{\text{noobj}}\sum_{i=0}^{S^2}\sum_{j=0}^B
\mathbb{1}^{noobj}_{ij}(C_i-\hat{C}_i)^2 & \text{无对象置信度误差} \\
&+
\sum_{i=0}^{S^2}\mathbb{1}^{obj}_{i}\sum_{c\in \text{classes} }
(C_i-\hat{C}_i)^2 & \text{对象分类误差} 
\end{align}
$$

* $\mathbb{1}^{obj}_{i}$表示目标出现在第$i$个网格中。
* $\mathbb{1}^{obj}_{ij}$第$i$个网格第$j$个边界框预中存在目标。
* $\mathbb{1}^{noobj}_{ij}$第$i$个网格第$j$个边界框中不存在目标。
* $\lambda_{\text{coord}}=5$增加位置误差的权重。
* $\lambda_{\text{noobj}}=0.5$减少不存在对象的bbox的置信度误差的权重。

输出结果与损失函数的对应关系

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/0cfdc372845a4c54d8afe738792d49df.png" style="zoom:55%;" />

### 模型训练

* 使用ImageNet数据集对前20层卷积网络进行预训练
* 使用完整的网络，在PASCAL VOC数据集上进行对象识别和定位的训练。
* 最后一层采用线性激活函数，其它层都是Leaky ReLU。
* 训练中采用了drop out和数据增强（data augmentation）来防止过拟合。

### 模型预测

* 将图片resize成$448\times448$的大小，送入到yolo网络中。
* 输出一个$7\times7\times30$ 的张量。
* 采用NMS算法选出最有可能是目标的结果。

### YOLO算法特点

优点

* 速度非常快，处理速度可以达到45fps。快速版本（网络较小）甚至可以达到155fps。
* 训练和预测可以端到端的进行，非常简便。

缺点

* 准确率会打折扣。
* 对于小目标和靠的很近的目标检测效果并不好。

## YOLO V2

YOLO V2在处理速度、预测准确率、识别对象这三个方面进行了改进，识别扩展到9000种，也称为YOLO9000。 

相关论文：[YOLO9000: Better, Faster, Stronger](https://arxiv.org/pdf/1612.08242)

### YOLO V2的改进

#### 批标准化（Batch Normalization）

批标准化，有助于解决反向传播过程中，梯度消失和梯度爆炸问题，降低对一些超参数的敏感性，从而能够获得更好的收敛速度和收敛效果。在YOLO V2中卷积后，全部加入BN层，网络会提升2%的mAP。

#### 使用高分辨率图像微调

YOLO V2中采用$224\times224$ 的图像预训练模型后，再采用$448\times448$的高分辨率样本，对分类模型进行微调（10个epoch），使网络特征逐渐适应$448\times448$的分辨率。然后再使用$448\times448$的样本进检测。

#### 采用Anchor Boxes

YOLO V2将网格数量提升为$13\times13$，每个网格采用5个先验框，总共有$13\times13\times5=845$个先验框。

Faster-Rcnn选择的anchor比例是认为指定的，YOLO V2是对训练集中，标注边框进行聚类分析，以寻找尽可能匹配样本的边框尺寸。最终选择，聚类的五种尺寸最为anchor box。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/cdc004762e0c37775e25bda0e2d8ccf1.png" style="zoom:55%;" />

#### 边框位置的预测

YOLO V2将边框的结果约束在特定的网格中

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/daa35f8d8258b00ad4464e356edf89a8.png" style="zoom:45%;" />

* $c_x$，$c_y$当前网格左上角到图像左上角的距离，$t_x$、$t_y$是中心位置的补偿量。
* $p_w$，$p_h$是先验框的宽和高，$t_w$、$t_h$为宽高的补偿量。
* $b_x$、$b_y$、$b_w$、$b_h$是预测边框的中心和宽高。

除位置信息外还有预测框的置信度
$$
\text{Pr}(\text{Object})\times\text{IOU}(b,\text{Object})=\sigma(t_0)
$$

#### 细粒度特征融合

YOLO V2中，引入passthrough层的方法，在特征图中保留一些细节信息。在最后一个pooling之前，特征图的大小是$26\times26\times512$，将其1拆4，直接传递到pooling后的特征图，两者叠加到一起作为输出的特征图。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/dce8cbd04ed68048db0b2f191444fe15.png" style="zoom:65%;" />

对于一张特征图拆分如下

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/aa1720a3228c8ff012ed7ec3ef757779.png" style="zoom:45%;" />

#### 多尺度训练

YOLO V2中没有全连接层，可以输入任何尺寸的图像。因为整个网络下采样倍数是32，训练时每10个batch就随机更换一种尺寸，使网络能够适应各种大小的对象检测。

#### 速度更快

YOLO V2选用了Darknet-19网络，作为特征提取网络。DarkNet-19比VGG-16小一些，精度不弱于VGG-16，但浮点运算量减少到$20\%$，保证更快的运算速度。YOLO V2的网络中只有卷积池化层，去掉了全连接层。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/v2-93811e6a2a7a84874ee17a797e08c5b8_1440w.jpg" style="zoom:70%;" />

#### 识别对象更多

VOC数据集可以检测20种对象，但实际上对象的种类非常多。YOLO V2利用ImageNet非常大量的分类样本，联合COCO的对象检测数据集一起训练，使得YOLO V2即使没有学过很多对象的检测样本，也能检测出这些对象。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/V2_Feature_Vector_2.png" style="zoom:55%;" />

* C为类别的数量，对于9000个类别建立了WordTree，虽然是9000个类别，但不是9000维向量。

## YOLO V3

YOLO V3是相比之前的算法，在小目标检测上，精度有显著提升。其主要改进有：

1. 利用多尺度特征进行目标检测。
2. 先验框更丰富。
3. 调整了网络结构。
4. 对象分类使用logistic代替了softmax，更适用于多标签分类任务。

YOLO V3对于每一幅输入图像，会预测三个不同尺度的输出，以检测出不同大小的目标。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/44bc9e0f547c47c526f9142a5625ee71.png" style="zoom:75%;" />

相关论文：[YOLOv3: An Incremental Improvement](https://arxiv.org/pdf/1804.02767)

### YOLO V3的改进

#### 多尺度检测

通常一幅图像包含各种不同的物体，且有大有小。理想情况，是一次就可以将所有大小的物体，同时检测出来。因此，网络必须具备能够“看到”不同大小，物体的能力。因为网络越深，特征图就会越小，小的物体也就越难检测出来。

在实际的特征图中，随着网络深度的加深，浅层的特征图，主要包含低级的信息（物体边缘、颜色、位置等初级信息），深层的特征图中包含高等信息（物体的语义信息，如：狗、猫、汽车等）。因此在不同级别的特征图，对应不同的尺度，可以在不同的级别中，进行目标检测。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/e6f97aa311e7979d6f317aae515c4005.png" style="zoom:55%;" />

1. 这种方法，首先建立图像金字塔，不同尺度的图像被输入到对应的网络当中，用于目标检测。但这样做的结果就是每个级别的金字塔都需要进行一次处理，速度很慢。
2. 只在最后一层特征图进行检测，这个结构无法检测不同大小的物体。
3. 对不同深度的特征图分别进行目标检测。这样小的物体会在浅层的特征图中被检测出来，而大的物体会在深层的特征图被检测出来，从而检测出不同尺度的物体，缺点是每一个特征图获得的信息，仅来源于之前的层，之后的层的特征信息加以利用。
4. 当前层的特征图，会对未来层的特征图进行上采样，并加以利用。因为有了这样一个结构，当前的特征图就可以获得“未来”层的信息，这样的话低阶特征与高阶特征就有机融合起来了，提升检测精度。YOLO V3中，就是采用这种方式。

#### 网络结构

YOLO V3采用了Darknet-53的网络结构（含有53个卷积层），它借鉴了残差网络ResNet的做法，在层之间设置了shortcut，来解决深层网络梯度的问题。整个结构里，没有池化层和全连接层，网络的下采样是通过设置卷积的stride为2来达到，每当通过这个卷积层之后图像的尺寸就会减小到一半。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/ff6d9943d430943311be2ebedb36defc.png" style="zoom:45%;" />

* CBL：由Conv+Bn+Leaky_relu激活函数三者组成。 
* Concat：张量拼接，会扩充两个张量的维度。

#### 先验框

YOLO V3采用K-means聚类得到先验框的尺寸，为每种尺度设定3种先验框，总共聚类出9种尺寸的先验框。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/f0ea2815b99c00c1df4cabe3e3652276.png" style="zoom:35%;" />

使用小的特征图检测大的物体，使用大的特征图来检测小的物体。9种先验框的直观比较

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/9ae8defe6e6ac02afa277c3abb4bc6e1.png" style="zoom:35%;" />

* 蓝色框为聚类得到的先验框。
* 黄色框为为标注数据。
* 红框是对象中心点所在的网格。

> [!warning]
>
> $52\times 52$是8倍下采样，当目标小于8个像素时，无法被检测出。

#### 逻辑回归

逻辑回归回归用于二分类问题，在YOLO V3中采用了这种分类方式。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/logistic_regression_schematic.png" style="zoom:65%;" />

使用softmax层的时候，已经假设每个输出，只对应某一个类别，但是在某些类别存在重叠情况（例如：woman和person）的标签，使用softmax就不能使网络，对数据进行很好的预测。

#### 输入与输出

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/95121f1cdf85555cc62a29b4e35c0aeb.png" style="zoom:40%;" />

YOLO V3使用coco数据集训练包含80个类别。

## YOLO V4

YOLO之父在2020年初宣布退出CV界，YOLO V4之后的作者并不是YOLO系列的原作者。YOLO V4总结了大部分检测技巧，然后经过筛选，排列组合，挨个实验。总体来说，YOLO V4并没有新的改进，而是使用了大量的目标检测的技巧。

相关论文：[YOLOv4: Optimal Speed and Accuracy of Object Detection](https://arxiv.org/pdf/2004.10934)

YOLO V4网络结构如下

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/91aab4def6471db7e2ac6ffa235e1cfa.png" style="zoom:65%;" />

* CBM：YOLO V4网络结构中的最小组件，由Conv+Bn+Mish激活函数三者组成。
* CBL：由Conv+Bn+Leaky_relu激活函数三者组成。
* Res unit：借鉴Resnet网络中的残差结构，让网络可以构建的更深。
* CSPX：由三个卷积层和X个Res unint模块Concate组成。
* SPP：采用$1\times1$，$5\times5$，$9\times9$，$13\times13$的最大池化的方式，进行多尺度融合。
* Concat：张量拼接，维度会扩充，和YOLO V3中一样。
* Add：张量相加，不会扩充维度。
* Backbone中卷积层的数量为72个。