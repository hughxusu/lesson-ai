## AlexNet

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/4991bc8301e20262de72bb180d9a48ae.png" style="zoom:60%;" />

2012年，AlexNet横空出世，该模型的名字源于论文第一作者的姓名Alex Krizhevsky 。AlexNet使用了8层卷积神经网络，以大优势赢得了ImageNet 2012图像识别挑战赛，它首次证明了学习到的特征可以超越人工设计的特征。

相关论文：[ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)

AlexNet与LeNet的设计理念非常相似

![](https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/3fc71d5fdb1df3490e48eed27566de8b.png)

该网络的特点是：

1. AlexNet包含8层变换，有5层卷积和2层全连接隐藏层，以及1个全连接输出层。一共输出层是1000类。
2. AlexNet第一层中的卷积核形状是11×11。第二层中的卷积核形状减小到5×5，之后全采用3×3。所有的池化层窗口大小为3×3、步幅为2的最大池化。
3. AlexNet将sigmoid激活函数改成了ReLU激活函数，解决了Sigmoid在网络层次较深时的梯度消失问题。特征图的rule函数如下

```python
[[-1, 2, -3],     [[0, 2, 0],
 [4, -5, 6],       [4, 0, 6],
 [-7, 8, -9]]      [0, 8, 0]]
```

4. AlexNet通过dropout来控制全连接层的模型复杂度。
5. 使用重叠的最大池化，此前的CNN网络多使用平均池化，最大池化避免了平均池化的模糊效果。
6. 使用进行局部正规化LRN层，局部神经元活动创建竞争机制，响应比较大的值变得更大，抑制其他反馈小的神经元，增加了泛华能力。

<img src="/Users/hughxusu/projects/lesson-ai/_images/cv/lrn.jpeg" style="zoom:65%;" />

   计算公式如下
$$
b_{x,y}^i=\frac{a_{x,y}^i}
{\left ( k+\alpha\sum_{j=\max(0,i-\frac{n}{2})}^{\min(N-1,i+\frac{n}{2})}\left (a^j_{x,y}\right)^2 \right )^\beta}
$$

7. 使用CUDA加速深度卷积网络的训练。

8. 数据增强，随机从256$\times$256的原始图像中截取227$\times$227大小的区域，以及水平翻转的镜像，相当于增加了256-224倍的数据量。

## 构建AlexNet

### 卷积层的封装

在Pytorch中定义网络，可以先构造子层，再用子层构造需要的网络。在AlexNet中统一使用Relu激活函数，全连接层增加了Dropout，针对这连个功能可以定义子层

```python
from torch import nn

class ConvRelu(nn.Module):
    def __init__(self, *args, **kwargs):
        super(ConvRelu, self).__init__()
        self.conv = nn.Conv2d(*args, **kwargs)
        self.relu = nn.ReLU()
        nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')
        nn.init.zeros_(self.conv.bias)

    def forward(self, x):
        return self.relu(self.conv(x))


class LinerRelu(nn.Module):
    def __init__(self, *args, dropout=0.5, **kwargs):
        super(LinerRelu, self).__init__()
        self.lin = nn.Linear(*args, **kwargs)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        nn.init.xavier_normal_(self.lin.weight)
        nn.init.zeros_(self.lin.bias)

    def forward(self, x):
        return self.dropout(self.relu(self.lin(x)))
```

* 封装卷积模块的时候，使用He初始化函数对卷积层初始化。
* 封装全连接模块的时候，使用Xavier初始化函数对全连接层进行初始化。
* 初始偏置一律设置为0，这样实现了对于不同层函数，选择不同的初始化方法。

> [!warning]
>
> 对于深层次模型训练时出现不收敛的情况，很可能是初始化权重选择错误造成的，可以尝试不同的优化权重。

### 定义AlexNet网络

使用上面封装的卷积和全连接层构建网络

```python
class AlexNet(nn.Module):
    def __init__(self):
        super(AlexNet, self).__init__()
        self.c1 = ConvRelu(1, 96, kernel_size=11, stride=4)
        self.p1 = nn.MaxPool2d(3, 2)
        self.c2 = ConvRelu(96, 256, kernel_size=5, padding=2)
        self.p2 = nn.MaxPool2d(3, 2)
        self.c3 = ConvRelu(256, 384, kernel_size=3, padding=1)
        self.c4 = ConvRelu(384, 384, kernel_size=3, padding=1)
        self.c5 = ConvRelu(384, 256, kernel_size=3, padding=1)
        self.p3 = nn.MaxPool2d(3, 2)
        self.flatten = nn.Flatten()
        self.l1 = LinerRelu(6 * 6 * 256, 4096)
        self.l2 = LinerRelu(4096, 4096)
        self.l3 = LinerRelu(4096, 10)

    def forward(self, x):
        x = self.p1(self.c1(x))
        x = self.p2(self.c2(x))
        x = self.p3(self.c5(self.c4(self.c3(x))))
        x = self.flatten(x)
        x = self.l3(self.l2(self.l1(x)))
        return x
```

打印模型信息

```python
from torchinfo import summary
model = AlexNet()
summary(model, (1, 1, 227, 227))
```

模型结构如下

```shell
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
AlexNet                                  [1, 10]                   --
├─ConvRelu: 1-1                          [1, 96, 55, 55]           --
│    └─Conv2d: 2-1                       [1, 96, 55, 55]           11,712
├─MaxPool2d: 1-2                         [1, 96, 27, 27]           --
├─ConvRelu: 1-3                          [1, 256, 27, 27]          --
│    └─Conv2d: 2-3                       [1, 256, 27, 27]          614,656
├─MaxPool2d: 1-4                         [1, 256, 13, 13]          --
├─ConvRelu: 1-5                          [1, 384, 13, 13]          --
│    └─Conv2d: 2-5                       [1, 384, 13, 13]          885,120
├─ConvRelu: 1-6                          [1, 384, 13, 13]          --
│    └─Conv2d: 2-7                       [1, 384, 13, 13]          1,327,488
├─ConvRelu: 1-7                          [1, 256, 13, 13]          --
│    └─Conv2d: 2-9                       [1, 256, 13, 13]          884,992
├─MaxPool2d: 1-8                         [1, 256, 6, 6]            --
├─Flatten: 1-9                           [1, 9216]                 --
├─LinerRelu: 1-10                        [1, 4096]                 --
│    └─Linear: 2-11                      [1, 4096]                 37,752,832
├─LinerRelu: 1-11                        [1, 4096]                 --
│    └─Linear: 2-14                      [1, 4096]                 16,781,312
├─LinerRelu: 1-12                        [1, 10]                   --
│    └─Linear: 2-17                      [1, 10]                   40,970
```

从上面的结构可以看出模型的参数最主要集中在全连接层。上面的模型时标准的AlexNet但是其训练周期过长，为了简化实验过程，对输入图像的尺寸进行压缩，选择$67\times67$的输入图像作为训练数据，模型定义为

```python
class AlexNetSmall(nn.Module):
    def __init__(self, dropout=0.5):
        super(AlexNetSmall, self).__init__()
        self.c1 = ConvRelu(1, 96, kernel_size=11, stride=4)
        self.p1 = nn.MaxPool2d(3, 2)
        self.c2 = ConvRelu(96, 256, kernel_size=5, padding=2)
        self.p2 = nn.MaxPool2d(3, 2)
        self.c3 = ConvRelu(256, 384, kernel_size=3, padding=1)
        self.c4 = ConvRelu(384, 384, kernel_size=3, padding=1)
        self.c5 = ConvRelu(384, 256, kernel_size=3, padding=1)
        self.p3 = nn.MaxPool2d(3, 2)
        self.flatten = nn.Flatten()
        self.l1 = LinerRelu(256, 128, dropout=dropout)
        self.l2 = LinerRelu(128, 128, dropout=dropout)
        self.l3 = LinerRelu(128, 10)

    def forward(self, x):
        x = self.p1(self.c1(x))
        x = self.p2(self.c2(x))
        x = self.p3(self.c5(self.c4(self.c3(x))))
        x = self.flatten(x)
        x = self.l3(self.l2(self.l1(x)))
        return x
```

打印模型机构

```python
model_small= AlexNetSmall()
summary(model_small, (1, 1, 67, 67))
```

当输入图像是$67\times67$的大小时，模型结构为

```python
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
AlexNetSmall                             [1, 10]                   --
├─ConvRelu: 1-1                          [1, 96, 15, 15]           --
│    └─Conv2d: 2-1                       [1, 96, 15, 15]           11,712
├─MaxPool2d: 1-2                         [1, 96, 7, 7]             --
├─ConvRelu: 1-3                          [1, 256, 7, 7]            --
│    └─Conv2d: 2-3                       [1, 256, 7, 7]            614,656
├─MaxPool2d: 1-4                         [1, 256, 3, 3]            --
├─ConvRelu: 1-5                          [1, 384, 3, 3]            --
│    └─Conv2d: 2-5                       [1, 384, 3, 3]            885,120
├─ConvRelu: 1-6                          [1, 384, 3, 3]            --
│    └─Conv2d: 2-7                       [1, 384, 3, 3]            1,327,488
├─ConvRelu: 1-7                          [1, 256, 3, 3]            --
│    └─Conv2d: 2-9                       [1, 256, 3, 3]            884,992
├─MaxPool2d: 1-8                         [1, 256, 1, 1]            --
├─Flatten: 1-9                           [1, 256]                  --
```

训练上面这个规模较小的AlexNet，导入封装的函数

```python
%run utils.py
```

### 模型训练

使用Fashion-MNIST数据集验证AlexNet模型，加载数据并划分验证集与训练集

```python
from torchvision import datasets, transforms
from utils import train_val_split

full = datasets.FashionMNIST(root='./data', train=True, download=True)
test = datasets.FashionMNIST(root='./data', train=False, download=True)
train, valid = train_val_split(full)

print(len(train), len(valid), len(test))
```

定义图像处理器，将图像放大到67$\times$67的区域

```python
from utils import PackDataset

trans = transforms.Compose([transforms.Resize(size=67), transforms.ToTensor()])
train_data = PackDataset(train, transform=trans)
valid_data = PackDataset(valid, transform=trans)
test_data = PackDataset(test, transform=trans)

image, label = train_data[0]
print(image.size())
```

将skorch训练过程中的回调函数封装在一起

```python
from skorch.callbacks import EarlyStopping, Checkpoint, EpochScoring, LRScheduler, ProgressBar
from torch.optim.lr_scheduler import CosineAnnealingLR

def control_callbacks(
        epochs, show_bar=True,
        model_name='best_model.pt', check_dir='./data/checkpoints'
    ):
    bar = ProgressBar()
    lr_scheduler = LRScheduler(policy=CosineAnnealingLR, T_max=epochs)
    early_stopping = EarlyStopping(monitor='valid_acc', lower_is_better=False, patience=6)
    train_acc = EpochScoring(name='train_acc', scoring='accuracy', on_train=True)
    check_point = Checkpoint(
        dirname=check_dir, f_params=model_name,
        monitor='valid_acc_best', load_best=True
    )
    calls = []
    if show_bar:
        calls.append(bar)
    calls.extend([lr_scheduler, early_stopping, train_acc, check_point])
    return calls
```

训练模型代码如下

```python
import torch
from skorch import NeuralNetClassifier
from skorch.helper import predefined_split

epochs = 50
calls = control_callbacks(epochs, check_dir='./data/alex-checkpoints')
net = NeuralNetClassifier(
    AlexNetSmall,
    criterion=nn.CrossEntropyLoss,
    optimizer=torch.optim.Adam,
    lr=0.001,
    batch_size=2048,
    max_epochs=epochs,
    train_split=predefined_split(valid_data),
    device='cuda' if torch.cuda.is_available() else 'cpu',
    callbacks=calls,
    classes=list(range(10)),
)
net.fit(X=train_data, y=None)
```

* 使用`NeuralNetClassifier`训练模型时，`fit`函数`X`传入的数据如果是`dataset`数据类型，`y`标签可以不用传入数据。但需要增加参数`classes=list(range(10))`，预先给模型设置分类的数量。

> [!warning]
>
> `batch_size`对收敛有一定影响，选`batch_size`时可以由小到大依次尝试，在收敛的同时尽量选择大一些的`batch_size`。

将损失函数曲线和准确率曲线绘制过程封装成函数，打印准确率曲线

```python
import matplotlib.pyplot as plt

def plot_history(net):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

    ax1.plot(net.history[:, 'train_loss'], label='Train Loss', linewidth=3)
    ax1.plot(net.history[:, 'valid_loss'], label='Valid Loss', linewidth=3)
    ax1.set_xlabel('Epoch', fontsize=14)
    ax1.set_ylabel('Loss', fontsize=14)
    ax1.set_title('Training & Validation Loss', fontsize=16)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    ax1.legend()

    ax2.plot(net.history[:, 'train_acc'], label='Train Accuracy', linewidth=3)
    ax2.plot(net.history[:, 'valid_acc'], label='Valid Accuracy', linewidth=3)
    ax2.set_xlabel('Epoch', fontsize=14)
    ax2.set_ylabel('Accuracy (%)', fontsize=14)
    ax2.set_title('Validation Accuracy', fontsize=16)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    ax2.legend()

    plt.tight_layout() 
    plt.show()

plot_history(net)
```

将测试集运行和结果统计过程封装成函数，测试模型性能

```python
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
import seaborn as sns
import numpy as np

def check_result(net, test_data):
    y_pred = net.predict(test_data) 
    y_prob = net.predict_proba(test_data) 
    y_true = np.array([y for x, y in iter(test_data)])     
    test_accuracy = accuracy_score(y_true, y_pred)
    print(f"Test Accuracy: {test_accuracy:.4f}")
    print('='*100)

    cm = confusion_matrix(y_true, y_pred)
    print("Confusion Matrix:\n", cm)
    plt.figure(figsize=(12, 10))
    sns.heatmap(
        cm, 
        annot=True, 
        fmt="d", 
        cmap="Blues",
        annot_kws={"size": 10},
    )
    plt.xlabel("Predicted Label", fontsize=14)
    plt.ylabel("True Label", fontsize=14)
    plt.title("Confusion Matrix (Test Set)", fontsize=16)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.show()
    print('='*100)
    y_hat = np.asarray(y_true)                 
    wrong_idx = np.where(y_pred != y_hat)[0]
    error_list = []
    for i in wrong_idx:
        features, _ = test_data[i]                  
        error_list.append({
            "features": features,              
            "true_label": int(y_hat[i]),
            "pred_label": int(y_pred[i]),
            "probabilities": y_prob[i]      
        })

    print(f'error number: {len(error_list)}')
    return error_list
  
check_result(net, test_data, test)
```

## 网格搜索超参数

在AlexNet训练过程中包含三个超参数：`lr`、`batch_size`和模型的`dropout`。为了挑选合适的超参数训练模型可以使用网格搜索的方法寻找最优超参数。

```python
import json
from sklearn.model_selection import ParameterGrid

epochs = 20
param_grid = {
    'lr': [0.001, 0.0005, 0.0001],
    'batch_size': [128, 256, 512, 1024, 2048],
    'dropout': [0.5, 0.3, 0.2]
}

results = {
    'best_params': None,
    'best_acc': 0.0,
    'all_results': []
}

calls = control_callbacks(epochs, check_dir='./data/alex-checkpoints', show_bar=False)
for params in ParameterGrid(param_grid):
    print(f"\nTraining with params: {params}")
    alex = AlexNetSmall(params['dropout'])

    net = NeuralNetClassifier(
        AlexNetSmall,
        criterion=nn.CrossEntropyLoss,
        optimizer=torch.optim.Adam,
        lr=params['lr'],
        batch_size=params['batch_size'],
        max_epochs=epochs,
        train_split=predefined_split(valid_data),
        device='cuda' if torch.cuda.is_available() else 'cpu',
        callbacks=calls,
        classes=list(range(10)),
    )
    net.fit(X=train_data, y=None)
    valid_acc = max(net.history[:, 'valid_acc'])
    current_result = {'params': params, 'valid_acc': valid_acc}
    results['all_results'].append(current_result)

    if valid_acc > results['best_acc']:
        results['best_acc'] = valid_acc
        results['best_params'] = params

    print(f"\nBest params: {results['best_params']}, best acc: {results['best_acc']}")

with open('./data/hyperparam_results.json', 'w') as f:
    json.dump(results, f, indent=2)
```

搜索超参数时，到达绝对收敛，可以选择部分数据执行20~50个epoch，通过`valid_acc`的比较选择，最佳超参数组合。根据上面的程序穿着最优参数`lr=0.0005`，`batch_size=2048`，`dropout=0.3`。修改训练代码如下

```python
epochs = 50
calls = control_callbacks(epochs, check_dir='./data/alex-checkpoints')
alex = AlexNetSmall(0.3)
net = NeuralNetClassifier(
    alex,
    criterion=nn.CrossEntropyLoss,
    optimizer=torch.optim.Adam,
    lr=0.0005,
    batch_size=2048,
    max_epochs=epochs,
    train_split=predefined_split(valid_data),
    device='cuda' if torch.cuda.is_available() else 'cpu',
    callbacks=calls,
    classes=list(range(10)),
)
net.fit(X=train_data, y=None)
```

绘制损失曲线和准确率曲线

```python
plot_history(net)
```

检查测试集的准确率

```python
check_result(net, test_data)
```

