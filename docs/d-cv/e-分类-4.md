# GoogLeNet

GoogLeNet在加深度的同时做了结构上的创新，引入了一个叫做Inception的结构来代替之前的卷积加激活的经典组件。GoogLeNet在ImageNet分类比赛上的Top-5错误率降低到了6.7%。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/1f2229e9e94fe17681cf755e74485050.png" style="zoom:60%;" />

相关论文：[Going Deeper with Convolutions](https://arxiv.org/pdf/1409.4842)

### Inception结构

GoogLeNet中的基础卷积块叫作Inception块，结构如下

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/fc86defb385e25d5f3948bf08861aaf4.png" style="zoom:70%;" />

Inception块里有4条并行的线路。最后，将每条线路的输出在通道维上连结，并向后进行传输。其中，$1\times1$的卷积，没有考虑在特征图局部信息之间的关系，它的作用主要是：

* 实现跨通道的交互和信息整合。
* 卷积核通道数的降维和升维，减少网络参数。
* $1\times1$卷积核后通常跟随激活函数，因此它不仅是一个线性变换，还引入了非线性，增强了模型的表达能力。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/db8ab22bff7282ebe72e11d7d9d0db63.png" style="zoom:70%;" />

1. 直接使用$3\times3$卷积参数量：$3\times3\times192\times32=55296$
2. 使用$1\times1$卷积核，再使用$3\times3$，计算公式：$1\times1\times192\times32+3\times3\times32\times32=15360$

定义Inception块如下

```python
class Inception(nn.Module):
    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):
        super().__init__()
        self.branch1 = conv_block(in_channels, ch1x1, conv_size=1)
        
        self.branch2 = nn.Sequential(
            conv_block(in_channels, ch3x3red, conv_size=1),
            conv_block(ch3x3red, ch3x3, conv_size=3, padding=1)
        )
        
        self.branch3 = nn.Sequential(
            conv_block(in_channels, ch5x5red, conv_size=1),
            conv_block(ch5x5red, ch5x5, conv_size=5, padding=2)
        )
        
        self.branch4 = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
            conv_block(in_channels, pool_proj, conv_size=1)
        )

    def forward(self, x):
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)
        return torch.cat([branch1, branch2, branch3, branch4], 1)
```

### 网络构建

[GoogLeNet 结构图](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_3.28.59_PM.png)

各个网络层参数

![](https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/googlenet-params.png)

1. B1模块

```python
self.conv_pool1 = conv_pool_block(3, 64, 7, 2, 3, 3, 2, 1)
```

2. B2模块

```python
self.conv2 = conv_block(64, 64, conv_size=1)
self.conv_pool2 = conv_pool_block(64, 192, 3, 1, 1, 3, 2, 1)
```

3. B3模块

```python
self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)
self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)
self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
```

4. B4模块

```python
self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)
self.aux1 = self._auxiliary_classifier(512, self.categories)
self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)
self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)
self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)
self.aux2 = self._auxiliary_classifier(832, self.categories)
self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)
self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
```

5. 输出模块

GoogLeNet使用全局平均池化层，来替代Flatten。将特征图每一通道中所有像素值相加后求平均，得到就是GAP的结果，在将其送入后续网络中进行计算。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/bc07bc99154507d1f4e1a009e32a7991.png" style="zoom:60%;" />

```python
self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)
self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)
self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
self.dropout = nn.Dropout(0.4)
self.fc = nn.Linear(1024, self.categories)
```

整体网络类

```python
class GoogleNet(ConfigModule):
    def __init__(self, model_config='config.yaml'):
        super().__init__(model_config)
        self.conv_pool1 = conv_pool_block(3, 64, 7, 2, 3, 3, 2, 1)
        self.conv2 = conv_block(64, 64, conv_size=1)
        self.conv_pool2 = conv_pool_block(64, 192, 3, 1, 1, 3, 2, 1)
        
        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)
        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)
        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)
        self.aux1 = self._auxiliary_classifier(512, self.categories)
        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)
        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)
        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)
        self.aux2 = self._auxiliary_classifier(832, self.categories)
        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)
        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)
        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(0.4)
        self.fc = nn.Linear(1024, self.categories)

    @staticmethod
    def _auxiliary_classifier(in_channels, num_classes):
        return nn.Sequential(
            nn.AvgPool2d(kernel_size=5, stride=3, padding=2),
            conv_block(in_channels, 128, conv_size=1),
            nn.Flatten(),
            dense_block(128, 1024, 0.7),
            nn.Linear(1024, num_classes)
        )

    def forward(self, x):
        x = self.conv_pool1(x)
        x = self.conv_pool2(self.conv2(x))
        
        x = self.inception3b(self.inception3a(x))
        x = self.maxpool3(x)

        x = self.inception4a(x)
        aux1 = self.aux1(x)

        x = self.inception4e(self.inception4d(self.inception4c(self.inception4b(x))))
        aux2 = self.aux2(x)
        x = self.maxpool4(x)

        x = self.inception5b(self.inception5a(x))
        x = nn.Flatten()(self.avgpool(x))
        x = self.dropout(x)
        x = self.fc(x)
        return x, aux1, aux2

    def _calculate(self, x, y):
        y_hat, aux1, aux2 = self(x)
        loss = self.loss_fn(y_hat, y) + self.loss_fn(aux1, y) * 0.3 + self.loss_fn(aux2, y) * 0.3
        return y_hat, loss
```

### 训练模型

训练模型

```python
trainer = pl.Trainer(
    max_epochs=100,
    accelerator="auto",
    devices="auto",
    deterministic=True
)
data = CIFAR10Data()
trainer.fit(google, datamodule=data)
```

打印评测结果

```python
trainer.test(google, datamodule=data)
```

绘制学习曲线

```python
from utils.plot import TrainingCurve
drawing = TrainingCurve('./lightning_logs/version_0/')
drawing.plot()
```

### Inception结构的改进

GoogLeNet是以InceptionV1为基础进行构建的，所以GoogLeNet也叫做InceptionNet，在随后的⼏年⾥，研究⼈员对GoogLeNet进⾏了数次改进， 就又产生了InceptionV2、V3、V4等版本。

1. InceptionV2。大卷积核拆分为小卷积核，将V1中的5x5的卷积用两个3x3的卷积替代，从而增加网络的深度，减少了参数。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/3c4a8fb6fe0839919d8f551bd7823440.png" style="zoom:45%;" />

2. InceptionV3。将n×n卷积分割为1×n和n×1两个卷积。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/6b59ec36f19062c0ea341887a99186be.png" style="zoom:50%;" />















 
