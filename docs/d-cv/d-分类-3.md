# VGGNet

2014年，牛津大学计算机视觉组（Visual Geometry Group）和Google DeepMind公司的研究员一起研发出了新的深度卷积神经网络：VGGNet，并取得了ILSVRC2014比赛分类项目的第二名，主要贡献是使用很小的卷积核(3×3)构建卷积神经网络结构，能够取得较好的识别精度，常用来提取图像特征的VGG-16和VGG-19。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/69b8ffeabd6f916d1f576f45cd6db623.png" style="zoom:60%;" />

相关论文：[Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/pdf/1409.1556)

VGG可以看成是加深版的AlexNet，整个网络由卷积层和全连接层叠加而成，和AlexNet不同的是，VGG中使用的都是小尺寸的卷积核。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/86ad60a39f7174e6bcee04f7e74ed1bc.png" style="zoom:67%;" />

VGGNet的主要改特点：

1. 更深的网络结构。VGGNet 的一个主要特点是其深度，通常有16到19层。通过增加网络深度，提高了模型的表达能力，可以学习到更复杂的特征。
2. 更小的卷积核。VGGNet 使用了3x3的卷积核。VGGNet认为，多个3x3的卷积核串联起来可以达到与大卷积核相同的感受野，但参数量更少，计算效率更高。
3. 连续的卷积层。VGGNet使用了多个连续的3x3卷积层。这种连续的卷积层可以更好地提取图像的局部特征，并减少参数量。
4. 更小的步幅。VGGNet使用了1个像素的步幅。更小的步幅可以保留更多的图像信息，提高模型的准确性。
5. 更多的池化层。VGGNet使用了更多的最大池化层，可以有效地减少特征图的尺寸，降低计算量，并提高模型的鲁棒性。
6. 使用ReLU激活函数，但去除了LRN层。
7. 基于Adam的最优化。
8. 使用He初始值作为权重初始值。

> [!warning]
>
> VGG网络最大的特点就是结构简洁，常用于各种后端应用的网络骨架。

## 构建网络

### 创建VGG模块

VGG网络可以看做由多个VGG模块构成，构建整个网络前，可以定义VGG的模块。定义VGG模块有两种方法

1. 使用`nn.Sequential`定义VGG模块

```python
from utils import ConvRelu
from torch import nn

def vgg_block(conv_in, conv_out, conv_num):
    layers = []
    layers.append(ConvRelu(conv_in, conv_out, kernel_size=3, padding=1))
    for i in range(conv_num - 1):
        layers.append(ConvRelu(conv_out, conv_out, kernel_size=3, padding=1))

    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
    return nn.Sequential(*layers)

block = vgg_block(3, 64, 2)
print(block)
```

* `conv_in`输入的特征图数量；`conv_out`输出的特征图数量；`conv_num` 堆叠的卷积层的数量。

2. 使用继承定义VGG模块

```python
class VggBlock(nn.Module):
    def __init__(self, conv_in, conv_out, conv_num):
        super(VggBlock, self).__init__()
        layers = []
        layers.append(ConvRelu(conv_in, conv_out, kernel_size=3, padding=1))
        for i in range(conv_num - 1):
            layers.append(ConvRelu(conv_out, conv_out, kernel_size=3, padding=1))

        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)

block = VggBlock(3, 64, 2)
print(block)
```

### 创建VGG模型

使用`VggBlock`来堆叠VGG16网络

```python
from utils import LinerRelu

class Vgg16(nn.Module):
    def __init__(self, dropout=0.5):
        super(Vgg16, self).__init__()
        self.block1 = VggBlock(3, 64, 2)
        self.block2 = VggBlock(64, 128, 2)
        self.block3 = VggBlock(128, 256, 3)
        self.block4 = VggBlock(256, 512, 3)
        self.block5 = VggBlock(512, 512, 3)
        self.fc1 = LinerRelu(512, 256, dropout=dropout)
        self.fc2 = LinerRelu(256, 128, dropout=dropout)
        self.fc3 = LinerRelu(128, 10, dropout=dropout)

    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.block4(x)
        x = self.block5(x)
        x = nn.Flatten()(x)
        x = self.fc1(x)
        x = self.fc2(x)
        x = self.fc3(x)
        return x
```

打印模型信息

```python
from torchinfo import summary

vgg16 = Vgg16()
summary(vgg16, (1, 3, 32, 32))
```

### 训练模型

使用Cifar-10的数据来训练VGG模型，导入数据集

```python
from torchvision import datasets
from utils import train_val_split

full = datasets.CIFAR10(root="./data", train=True, download=True)
test = datasets.CIFAR10(root="./data", train=False, download=True)
train, valid = train_val_split(full, seed=666)

print(len(train), len(valid), len(test))
```

将数据集转换为`dataset`

```python
from utils import PackDataset
from torchvision import transforms

train_data = PackDataset(train, transform=transforms.ToTensor())
valid_data = PackDataset(valid, transform=transforms.ToTensor())
test_data = PackDataset(test, transform=transforms.ToTensor())

image, label = train_data[0]
print(image.size())
```

搜索训练的最优超参数

```python
import json
from utils import control_callbacks
from sklearn.model_selection import ParameterGrid
import torch
from skorch import NeuralNetClassifier
from skorch.helper import predefined_split

epochs = 20
param_grid = {
    'lr': [0.0001, 0.00005, 0.00001],
    'dropout': [0.5, 0.3, 0.2]
}

results = {
    'best_params': None,
    'best_acc': 0.0,
    'all_results': []
}

calls = control_callbacks(epochs, check_dir='./data/alex-checkpoints', show_bar=False)
for params in ParameterGrid(param_grid):
    print(f"\nTraining with params: {params}")
    alex = Vgg16(params['dropout'])
    net = NeuralNetClassifier(
        alex,
        criterion=nn.CrossEntropyLoss,
        optimizer=torch.optim.Adam,
        lr=params['lr'],
        batch_size=2048,
        max_epochs=epochs,
        train_split=predefined_split(valid_data),
        device='cuda' if torch.cuda.is_available() else 'cpu',
        callbacks=calls,
        classes=list(range(10)),
    )
    net.fit(X=train_data, y=None)
    valid_acc = max(net.history[:, 'valid_acc'])
    current_result = {'params': params, 'valid_acc': valid_acc}
    results['all_results'].append(current_result)

    if valid_acc > results['best_acc']:
        results['best_acc'] = valid_acc
        results['best_params'] = params

    print(f"\nBest params: {results['best_params']}, best acc: {results['best_acc']}")

with open('./data/hyperparam_results.json', 'w') as f:
    json.dump(results, f, indent=2)
```

最优超参数为`lr=0.0001`，`dropout=0.3`



## 图像增强

为了在有限的数据集上增加训练数据容量，可以采用图像增强技术。

* 对图像进行不同方式的裁剪，物体出现在不同位置，从而减轻模型对物体位置的依赖性。
* 可以调整亮度、色彩等因素来降低模型对色彩的敏感度。

图像增强（image augmentation）指通过剪切、旋转、反射、翻转变换、缩放变换、平移变换、尺度变换、对比度变换、噪声扰动、颜色变换等，一种或多种组合变换来增加数据集的大小。通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模。同时，随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力

### 图像增强的方法

图像增强方式可以分为两类：几何变换类和颜色变换类。

1. 几何变换类。主要是对图像进行几何变换操作，包括：翻转，旋转，裁剪，变形，缩放等。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/fd12f33546a47d44ee32a40b4504fd78.png" style="zoom:55%;" />

2. 颜色变换类。指通过模糊、颜色变换、擦除、填充等方式对图像进行处理。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/e19639f94552c6d5d89f2aabda3d2999.png" style="zoom:50%;" />

图像增强在PyTorch中可以通过`transforms`来完成。

### `transforms.Compose`使用

`transforms.Compose`是在线的图像增强方法，可以在batch中对数据进行增强，扩充数据集大小，增强模型的泛化能力。

```python
transforms.Compose([
  transforms.Resize(size=67),   # 放大图像
  transforms.ToTensor()         # 将图像转换为tensor
])
```

### ` transforms`使用

显示67$\times$67的图像原始输入图像

```python
from utils import show_tensor_image

def show_trans(tans_example):
    train_data = PackDataset(full, transform=tans_example)
    image, _ = train_data[0]
    show_tensor_image(image)

tans_example = transforms.Compose([transforms.Resize(size=67), transforms.ToTensor()])
show_trans(tans_example)
```

1. 水平翻转`transforms.RandomHorizontalFlip`

```python
show_trans([
  transforms.RandomHorizontalFlip(1), transforms.Resize(size=67), transforms.ToTensor()
])
```

* 输入参数为转换概率一般设置为0.5。

2. 上下翻转`transforms.RandomVerticalFlip`

```python
show_trans([
  transforms.RandomVerticalFlip(1), transforms.Resize(size=67), transforms.ToTensor()
])
```

3. 随机裁剪`transforms.RandomResizedCrop`

```python
show_trans([
    transforms.RandomResizedCrop(size=67, scale=(0.6, 0.6), ratio=(1.0, 1.0)),
    transforms.ToTensor()
])
```

4. 随机旋转`transforms.RandomRotation`

```python
show_trans([transforms.RandomRotation(30), transforms.Resize(size=67), transforms.ToTensor()])
```

由于训练数据较小上面的旋转函数会导致图像被裁剪，因此封装一个缩放旋转函数，保证图像完整性。

```python
class RandomRotateExpandTransform:
    def __init__(self, degrees=30, fill=0, interpolation=Image.BICUBIC):
        if isinstance(degrees, (tuple, list)):
            self.min_angle, self.max_angle = degrees
        else:
            self.min_angle = -degrees
            self.max_angle = degrees

        self.fill = fill
        self.interpolation = interpolation

    def __call__(self, img: Image.Image) -> Image.Image:
        if not isinstance(img, Image.Image):
            raise TypeError("输入图像必须是 PIL.Image.Image 类型")

        orig_size = img.size
        angle = random.uniform(self.min_angle, self.max_angle)

        rotated = img.rotate(
            angle,
            resample=self.interpolation,
            expand=True,
            fillcolor=self.fill
        )

        resized = rotated.resize(orig_size, resample=self.interpolation)
        return resized

show_trans([RandomRotateExpandTransform(30), transforms.Resize(size=67), transforms.ToTensor()])
```

颜色变换，随机调整亮度、对比度、饱和度和色调。`transforms.ColorJitter`。



可以将将上述的一系列操作组合在一起可以实现图像增强

```python
trans_list = [
    transforms.RandomHorizontalFlip(0.2),
    transforms.RandomVerticalFlip(0.2),
    RandomRotateExpandTransform(25),
    transforms.RandomResizedCrop(size=67, scale=(0.8, 1), ratio=(1.0, 1.0)),
    transforms.ToTensor()
]
show_trans(trans_list)
```



