# VGGNet

2014年，牛津大学计算机视觉组（Visual Geometry Group）和Google DeepMind公司的研究员一起研发出了新的深度卷积神经网络：VGGNet，并取得了ILSVRC2014比赛分类项目的第二名，主要贡献是使用很小的卷积核(3×3)构建卷积神经网络结构，能够取得较好的识别精度，常用来提取图像特征的VGG-16和VGG-19。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/69b8ffeabd6f916d1f576f45cd6db623.png" style="zoom:60%;" />

相关论文：[Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/pdf/1409.1556)

VGG可以看成是加深版的AlexNet，整个网络由卷积层和全连接层叠加而成，和AlexNet不同的是，VGG中使用的都是小尺寸的卷积核。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/86ad60a39f7174e6bcee04f7e74ed1bc.png" style="zoom:67%;" />

VGGNet的主要改特点：

1. 更深的网络结构。VGGNet 的一个主要特点是其深度，通常有16到19层。通过增加网络深度，提高了模型的表达能力，可以学习到更复杂的特征。
2. 更小的卷积核。VGGNet 使用了3x3的卷积核。VGGNet认为，多个3x3的卷积核串联起来可以达到与大卷积核相同的感受野，但参数量更少，计算效率更高。
3. 连续的卷积层。VGGNet使用了多个连续的3x3卷积层。这种连续的卷积层可以更好地提取图像的局部特征，并减少参数量。
4. 更小的步幅。VGGNet使用了1个像素的步幅。更小的步幅可以保留更多的图像信息，提高模型的准确性。
5. 更多的池化层。VGGNet使用了更多的最大池化层，可以有效地减少特征图的尺寸，降低计算量，并提高模型的鲁棒性。
6. 使用ReLU激活函数，但去除了LRN层。
7. 基于Adam的最优化。
8. 使用He初始值作为权重初始值。

> [!warning]
>
> VGG网络最大的特点就是结构简洁，常用于各种后端应用的网络骨架。

## 构建网络

### 创建VGG模块

VGG网络可以看做由多个VGG模块构成，构建整个网络前，可以定义VGG的模块。定义VGG模块有两种方法

1. 使用`nn.Sequential`定义VGG模块

```python
from utils import ConvRelu
from torch import nn

def vgg_block(conv_in, conv_out, conv_num):
    layers = []
    layers.append(ConvRelu(conv_in, conv_out, kernel_size=3, padding=1))
    for i in range(conv_num - 1):
        layers.append(ConvRelu(conv_out, conv_out, kernel_size=3, padding=1))

    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
    return nn.Sequential(*layers)

block = vgg_block(3, 64, 2)
print(block)
```

* `conv_in`输入的特征图数量；`conv_out`输出的特征图数量；`conv_num` 堆叠的卷积层的数量。

2. 使用继承定义VGG模块

```python
class VggBlock(nn.Module):
    def __init__(self, conv_in, conv_out, conv_num):
        super(VggBlock, self).__init__()
        self.layers = []
        self.layers.append(ConvRelu(conv_in, conv_out, conv_num))
        for i in range(conv_num - 1):
            self.layers.append(ConvRelu(conv_out, conv_out, conv_num))

        self.layers.append(nn.MaxPool2d(kernel_size=2, stride=2))

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

block = VggBlock(3, 64, 2)
print(block)
```

### 创建VGG模型

使用`VggBlock`来堆叠网络

```python
class PackModule(LightningModule):
    def __init__(self, model_config='model_config.yaml'):
        super().__init__()
        self.config = open_config_file(model_config)
        self.loss_fn = get_loss_fn(self.config)
        self.categories = self.config.get('categories', 2)
        self.train_accuracy = Accuracy(task="multiclass", num_classes=self.categories)
        self.val_accuracy = Accuracy(task="multiclass", num_classes=self.categories)
        self.test_accuracy = Accuracy(task="multiclass", num_classes=self.categories)

    def configure_optimizers(self):
        optimizer = get_optimizer(self.parameters(), self.config)
        return optimizer

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)

        self.train_accuracy(y_hat, y)
        self.log("train_loss", loss, prog_bar=True)
        self.log("train_acc", self.train_accuracy, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)

        self.val_accuracy(y_hat, y)
        self.log("val_loss", loss, prog_bar=True)
        self.log("val_acc", self.val_accuracy, prog_bar=True)

    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)

        self.test_accuracy(y_hat, y)
        self.log("test_loss", loss)
        self.log("test_acc", self.test_accuracy, prog_bar=True)
```

定义VGG16模型

```python
class VGG16(PackModule):
    def __init__(self, model_config='model_config.yaml'):
        super().__init__()
        conv_arch = ((3, 64, 2), (64, 128, 2), (128, 256, 3), (256, 512, 3), (512, 512, 3))
        self.features = vgg_sequence(conv_arch)
        self.classifier = nn.Sequential(
            dense_block(512, 4096),
            dense_block(4096, 4096),
            nn.Linear(4096, self.categories),
        )

    def forward(self, x):
        x = self.features(x)
        x = nn.Flatten()(x)
        x = self.classifier(x)
        return x
```

训练模型

```python
trainer = pl.Trainer(
    max_epochs=10,
    accelerator="auto",
    devices="auto",
    deterministic=True
)
model = VGG16()
data = CIFAR10Data()
trainer.fit(model, datamodule=data)
```

测试模型

```python
trainer.test(model, datamodule=data)
```

绘制训练曲线

```python
model.plot_loss_acc_curve('./lightning_logs/version_1/')
```

## 图像增强

为了在有限的数据集上增加训练数据容量，可以采用图像增强技术。

* 对图像进行不同方式的裁剪，物体出现在不同位置，从而减轻模型对物体位置的依赖性。
* 可以调整亮度、色彩等因素来降低模型对色彩的敏感度。

图像增强（image augmentation）指通过剪切、旋转、反射、翻转变换、缩放变换、平移变换、尺度变换、对比度变换、噪声扰动、颜色变换等，一种或多种组合变换来增加数据集的大小。通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模。同时，随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力

### 图像增强的方法

图像增强方式可以分为两类：几何变换类和颜色变换类。

1. 几何变换类。主要是对图像进行几何变换操作，包括：翻转，旋转，裁剪，变形，缩放等。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/fd12f33546a47d44ee32a40b4504fd78.png" style="zoom:55%;" />

2. 颜色变换类。指通过模糊、颜色变换、擦除、填充等方式对图像进行处理。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/e19639f94552c6d5d89f2aabda3d2999.png" style="zoom:50%;" />

图像增强在PyTorch中可以通过`transforms`来完成。

### `transforms.Compose`使用

`transforms.Compose`是在线的图像增强方法，可以在batch中对数据进行增强，扩充数据集大小，增强模型的泛化能力。

```python
transforms.Compose([
  transforms.Resize(size=67),   # 放大图像
  transforms.ToTensor()         # 将图像转换为tensor
])
```

### ` transforms`使用

显示67$\times$67的图像原始输入图像

```python
from utils import show_tensor_image

def show_trans(tans_example):
    train_data = PackDataset(full, transform=tans_example)
    image, _ = train_data[0]
    show_tensor_image(image)

tans_example = transforms.Compose([transforms.Resize(size=67), transforms.ToTensor()])
show_trans(tans_example)
```

1. 水平翻转`transforms.RandomHorizontalFlip`

```python
show_trans([
  transforms.RandomHorizontalFlip(1), transforms.Resize(size=67), transforms.ToTensor()
])
```

* 输入参数为转换概率一般设置为0.5。

2. 上下翻转`transforms.RandomVerticalFlip`

```python
show_trans([
  transforms.RandomVerticalFlip(1), transforms.Resize(size=67), transforms.ToTensor()
])
```

3. 随机裁剪`transforms.RandomResizedCrop`

```python
show_trans([
    transforms.RandomResizedCrop(size=67, scale=(0.6, 0.6), ratio=(1.0, 1.0)),
    transforms.ToTensor()
])
```

4. 随机旋转`transforms.RandomRotation`

```python
show_trans([transforms.RandomRotation(30), transforms.Resize(size=67), transforms.ToTensor()])
```

由于训练数据较小上面的旋转函数会导致图像被裁剪，因此封装一个缩放旋转函数，保证图像完整性。

```python
class RandomRotateExpandTransform:
    def __init__(self, degrees=30, fill=0, interpolation=Image.BICUBIC):
        if isinstance(degrees, (tuple, list)):
            self.min_angle, self.max_angle = degrees
        else:
            self.min_angle = -degrees
            self.max_angle = degrees

        self.fill = fill
        self.interpolation = interpolation

    def __call__(self, img: Image.Image) -> Image.Image:
        if not isinstance(img, Image.Image):
            raise TypeError("输入图像必须是 PIL.Image.Image 类型")

        orig_size = img.size
        angle = random.uniform(self.min_angle, self.max_angle)

        rotated = img.rotate(
            angle,
            resample=self.interpolation,
            expand=True,
            fillcolor=self.fill
        )

        resized = rotated.resize(orig_size, resample=self.interpolation)
        return resized

show_trans([RandomRotateExpandTransform(30), transforms.Resize(size=67), transforms.ToTensor()])
```

颜色变换，随机调整亮度、对比度、饱和度和色调。`transforms.ColorJitter`。



可以将将上述的一系列操作组合在一起可以实现图像增强

```python
trans_list = [
    transforms.RandomHorizontalFlip(0.2),
    transforms.RandomVerticalFlip(0.2),
    RandomRotateExpandTransform(25),
    transforms.RandomResizedCrop(size=67, scale=(0.8, 1), ratio=(1.0, 1.0)),
    transforms.ToTensor()
]
show_trans(trans_list)
```



