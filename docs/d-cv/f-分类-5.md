# 残差网络

网络越深，获取的信息就越多，特征也越丰富。但是在实践中，随着网络的加深，优化效果反而越差，测试数据和训练数据的准确率反而降低了。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/8d17d4ed997300dad29d868b16039850.png" style="zoom:90%;" />

> [!note]
>
> 退化问题（Addressing Degradation Problem）
>
> 在深度神经网络中，随着网络深度的增加，理论上模型的能力应该增强，可以学习更复杂的特征。然而，实际情况是当网络深度增加到一定程度后，模型的性能不仅没有提升，反而开始下降，这种现象被称为**退化问题 **。退化问题**不是过拟合**，退化问题即使在训练集上，更深的网络也比更浅的网络表现更差。 这意味着更深的网络训练过程，遇到了困难。

针对这一问题，何恺明等人提出了残差网络残差网络（ResNet）在2015年的ImageNet图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/08803e5a9b2d589cc873657fd0903fef.png" style="zoom:60%;" />

相关论文：[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385)

## 残差层的原理

### 恒等映射（Identity Mapping）

假设在一个浅层网络上增加了一些层，深层网络性能至少和浅层网络一样，最差的情况是新增的这些层不影响网络的性能。换句话说，新增的层最差，也应该学习到**恒等映射**，即输入什么就输出什么，不引入任何额外的变换。

> [!warning]
>
> 如果新增的层能够学习到恒等映射，那么深层网络至少不会比浅层网络更差。  如果新增的层还能学习到有效的特征变换，那么深层网络就应该比浅层网络更好。

### 残差块（Residual Block）

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/4513e8ffd7dd54db4ff54d3c3a542c6c.png" style="zoom:60%;" />

假设卷积层是输入是$X$，训练神经网络的过程是找到一个理想的输出$H(X)$

* 理想的输出$H(X)$表示一种理想的特征图，以最优的方式满足后面网络的需要。
* 理想的特征图，最差的情况应该与$X$性能一致，不应该比$X$性能还差。

在普通的网络层中，学习目标是希望拟合出理想的输出$H(X)$。而在残差块中，改变了思路，让网络去学习残差函数$F(X) = H(X) - X$，学习目标是输出$H(X)$和输入$X$之间的差异（residual）。假设网络学习到了理想残差函数$F(X)$，最终的输出应该加上原始的输入，即$H(X) = F(X) + X$。所以，实际输出就变为$F(X)+X$。 

举个例子，假设要把一张略微模糊的猫的图片变得清晰（图像超分辨率任务）。

- 直接学习$H(X)$：让网络直接学习一个映射$H(X)$来输出清晰的猫的图片，网络需要从模糊的输入$X$中学习到清晰猫图片的所有特征，包括猫的轮廓、纹理、细节等等。
- 学习残差$F(X) = H(X) - X$：让网络学习模糊图像$X$和清晰图像$H(X)$之间的差异。  这个差异可能主要集中在图像的高频细节信息（例如，边缘更锐利，纹理更清晰）。网络只需要学习如何添加这些高频细节到模糊图像$X$上，就可以得到清晰的图像$H(X) = F(X) + X$。这显然比直接学习完整的清晰图像$H(X)$要更容易。

> [!warning]
>
> 这里的加法指的是元素级别的加法，将F(x)和原始输入的特征图对应元素相加。
>
> ```
> A =                B =                    C =
> [[1, 2, 3],        [[10, 11, 12],         [[1+10, 2+11, 3+12],
> [4, 5, 6],         [13, 14, 15],          [4+13, 5+14, 6+15],
> [7, 8, 9]]         [16, 17, 18]]          [7+16, 8+17, 9+18]]
> ```

### 学习残差的好处

如果希望网络层学习到恒等映射$H(X) = X$，那么网络层的权重需要精心调整，使得输出尽可能接近输入。  这对于深层网络来说是非常困难的，优化器很难找到合适的参数组合来精确地学习恒等映射。

在残差块中，如果理想的映射$H(X)$就是恒等映射，那么最优的情况就是让$F(X)$逼近于0。  将$F(X)$逼近于0，只需要让残差块中的卷积层权重趋近于零即可。

> [!warning]
>
> 残差模块学习恒等映射比，一般网络更容易。

### 残差模块的优点

* 解决退化问题。通过残差连接，即使深层网络中的某些层学习效果不佳，学习到的$F(x)$趋近于0，信息仍然可以通过shortcut连接无损地传递到后面的层。 这样就避免了信息在深层网络中逐渐衰减，缓解了退化问题。即使网络深度增加，性能也不会急剧下降，反而有可能因为更深的网络层能够学习到更复杂的特征而提升性能。

- 减缓梯度消失。残差连接提供了一条直接的梯度通路（gradient highway），梯度可以直接从深层传递到浅层，而无需完全依赖于中间层。
- 学习”差异“或”变化“。学习残差$F(X) = H(X) - X$ 意味着网络层不是从头开始学习全新的特征映射 $H(X)$，而是学习在输入$X$的基础上需要进行哪些“调整”或“变化”才能得到理想的输出$H(X)$。

### 残差层的设计

残差块有两种设计分别用于浅层网络和深层网络，深层网络中$1\times1$的卷积用于降维。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/renet-1.jpg" style="zoom:55%;" />

在残差网络中，残差块的输入$X$ 和残差分支$F(X)$必须有相同的形状，在一些情况下，残差块的输入维度和输出维度可能会发生变化，所以需要在shortcut连接上增加卷积。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/resnet-2.jpg" style="zoom:60%;" />

残差块中沿用了VGG的设计，用多层小卷积来代替大的卷积核。

## 网络构建

### 残差层的实现

实践证明在残差块中加入BN层效果会更好，浅层残差块设计如下

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/e7c3b6da09fcc5aea9f514d29eb4deb6.png" style="zoom:65%;" />



将BN层假如到卷积过程中，重新封装函数如下

```python
from torch import nn

class ConvBn(nn.Module):
    def __init__(self, *args, **kwargs):
        super(ConvBn, self).__init__()
        self.conv = nn.Conv2d(*args, **kwargs)
        self.bn = nn.BatchNorm2d(self.conv.out_channels)
        nn.init.kaiming_normal_(self.conv.weight)
        nn.init.zeros_(self.conv.bias)

    def forward(self, x):
        return self.bn(self.conv(x))

class ConvBnRelu(nn.Module):
    def __init__(self, *args, **kwargs):
        super(ConvBnRelu, self).__init__()
        self.conv = ConvBn(*args, **kwargs)
        self.relu = nn.ReLU()

    def forward(self, x):
        return self.relu(self.conv(x))
```

使用上面定义的层实现残差单元

```python
class ResUnit(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResUnit, self).__init__()
        self.conv1 = ConvBnRelu(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
        self.conv2 = ConvBn(out_channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.downsample = None
        if in_channels != out_channels or stride != 1:
            self.downsample = nn.Sequential(
                ConvBn(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.conv2(out)
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity
        return nn.ReLU()(out)


unit = ResUnit(64, 128, stride=2)
print(unit)
```

* `out_channels`残差块的卷积数（模块输出的通道数量），根据`stride`判断是否要添加卷积层，保持模型输入的尺寸一致。

### 创建残差网络

ResNet的层数主要指残差网络中包含的卷积层数量和全连接层数量，经典的残差网层数有：

* ResNet-18
* ResNet-34
* ResNet-50
* ResNet-101
* ResNet-152

ResNet-18结构图如下

![](https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/420d05bfd2718b5870d436f26db2b84d.png)

上图中多个`ResUnit`可以构成一个残差模块，定义残差模块

```python
class ResBlock(nn.Module):
    def __init__(self, in_channels, out_channels, num_units, stride=1):
        super(ResBlock, self).__init__()
        layers = []
        layers.append(ResUnit(in_channels, out_channels, stride=stride))
        for i in range(num_units - 1):
            layers.append(ResUnit(out_channels, out_channels, stride=1))
        self.blocks = nn.Sequential(*layers)

    def forward(self, x):
        return self.blocks(x)

block = ResBlock(64, 128, 2)
print(block)
```

* `num_residuals`残差块中包含几个残差层。
* `stride`控制卷积的步长。

构建残差网络

```python
class ResNet(nn.Module):
    def __init__(self):
        super(ResNet, self).__init__()
        self.conv1 = ConvBnRelu(3, 64, kernel_size=3, stride=1, padding=1)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.block1 = ResBlock(64, 64, 2)
        self.block2 = ResBlock(64, 128, 2, stride=2)
        self.block3 = ResBlock(128, 256, 2, stride=2)
        self.block4 = ResBlock(256, 512, 2, stride=2)
        self.pool2 = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.block4(x)
        x = self.pool2(x)
        x = nn.Flatten()(x)
        x = self.fc(x)
        return x

```

打印残差网络信息

```python
from torchinfo import summary

net = ResNet()
summary(net, (1, 3, 32, 32))
```

### 训练模型

加载CIFAR10数据

```python
from torchvision import datasets
from utils import train_val_split
from utils import PackDataset
from torchvision import transforms

full = datasets.CIFAR10(root="./data", train=True, download=True)
test = datasets.CIFAR10(root="./data", train=False, download=True)
train, valid = train_val_split(full, seed=666)

train_data = PackDataset(train, transform=transforms.ToTensor())
valid_data = PackDataset(valid, transform=transforms.ToTensor())
test_data = PackDataset(test, transform=transforms.ToTensor())
```

搜索最优参数

```python
import json
from utils import control_callbacks
from sklearn.model_selection import ParameterGrid
import torch
from skorch import NeuralNetClassifier
from skorch.helper import predefined_split

epochs = 20
param_grid = {
    'lr': [0.01, 0.005, 0.001, 0.0005, 0.0001],
}

results = {
    'best_params': None,
    'best_acc': 0.0,
    'all_results': []
}

calls = control_callbacks(epochs, check_dir='./data/alex-checkpoints', show_bar=False)
for params in ParameterGrid(param_grid):
    print(f"\nTraining with params: {params}")
    net = NeuralNetClassifier(
        ResNet,
        criterion=nn.CrossEntropyLoss,
        optimizer=torch.optim.Adam,
        lr=params['lr'],
        batch_size=2048,
        max_epochs=epochs,
        train_split=predefined_split(valid_data),
        device='cuda' if torch.cuda.is_available() else 'cpu',
        callbacks=calls,
        classes=list(range(10)),
    )
    net.fit(X=train_data, y=None)
    valid_acc = max(net.history[:, 'valid_acc'])
    current_result = {'params': params, 'valid_acc': valid_acc}
    results['all_results'].append(current_result)

    if valid_acc > results['best_acc']:
        results['best_acc'] = valid_acc
        results['best_params'] = params

    print(f"\nBest params: {results['best_params']}, best acc: {results['best_acc']}")

with open('./data/hyperparam_results.json', 'w') as f:
    json.dump(results, f, indent=2)
```



有论文表示残差网在CIFAR10准确率可以达到94%，[参考文章](https://blog.csdn.net/qq_41019681/article/details/109757387)。

## 模型微调