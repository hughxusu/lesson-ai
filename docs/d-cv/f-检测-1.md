# 图像检测

目标检测（Object Detection）的任务是找出图像中所有感兴趣的目标，并确定它们的类别和位置。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/4a343e56f3dc05a9a8a5799d8ea1910e.png" style="zoom:35%;" />

目标检测中能检测出来的物体，取决于当前任务定义的物体有哪些。假设目标检测模型定义检测目标为猫和狗，那么模型对任意一张图片都不会输出除猫和狗外的其它结果。

目标检测，一般以图片左上角为原点，位置信息一般有两种格式：

1. 极坐标表示：目标左上角（位置最小点）和目标右下角（位置最大点）。
2. 中心点坐标：目标中心点和物体的宽高。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/c7480bc43105cd61212bf4683a3c22a2.png" style="zoom:50%;" />

目标检测过程：

1. 对目标类别进行分类，为分类任务。
2. 定位目标的位置，回归任务。

## 开源数据集

经典的目标检测数据集有两种：PASCAL VOC数据集和MS COCO数据集。

### PASCAL VOC

PASCAL VOC包含约10000张带有边界框的图片用于训练和验证，是目标检测问题的一个基准数据集。常用的是VOC2007和VOC2012两个版本数据，共20个类别。

1. Person：person
2. Animal：bird、cat、cow、dog、horse、sheep
3. Verhical：aeroplane、bicycle、boat、bus、car、motorbike、train
4. Indoor：bottle、chair、dining table、potted plant（盆栽）、sofa、tv/monitor

[PASCAL VOC 下载](https://pjreddie.com/projects/pascal-voc-dataset-mirror/) TensorFlow中没有PASCAL VOC数据集

```shell
.
├── Annotations               标注信息
├── ImageSets                 指定图片的训练和验证
│   ├── Action                人的动作（running、jumping等）
│   ├── Layout                具有人体部位的数据（人的head、feet等）
│   ├── Main                  图像物体识别的数据
│   │   ├── train.txt         训练集的图片列表
│   │   └── val.txt						验证集的图片列表
│   └── Segmentation          可用于分割的数据
├── JPEGImages                图片信息
├── SegmentationClass         
└── SegmentationObject
```

* Main下存放的是图像物体识别的数据，总共分为20类，这是进行目标检测的重点。该文件夹中的数据对负样本文件进行了描述。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/f6072be38463afeed7f0ea5d0cbefb98.png" style="zoom:60%;" />

PASCAL VOC的标注信息

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/4e59f4ef1d1673ae501cae5cc9fb46e3.png" style="zoom:60%;" />

### MS COCO

MS COCO（Microsoft Common Objects in Context）微软于2014年出资标注的Microsoft COCO数据集，与ImageNet竞赛一样，被视为是计算机视觉领域最受关注和最权威的比赛之一。COCO数据集提供的类别有80类，有超过33万张图片，其中20万张有标注，整个数据集中个体的数目超过150万个。
[MS COCO 首页](https://cocodataset.org/#home)

## 常用评价指标

在目标检测算法中，IOU（intersection over union交并比）是目标检测算法中用来评价2个矩形框之间相似度的指标。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/40d1aed8f50ba8fd27549f2e1476d369.png" style="zoom:50%;" />

实际图片中的应用

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/ef92ed26e169401ea0c0e2c6cd75ccad.png" style="zoom:50%;" />

一般情况下对于检测框的判定都会存在一个阈值，当IOU的值大于0.5的时候，则可认为检测到目标物体。IOU计算代码如下

```python
import numpy as np

def Iou(box1, box2, wh=False):
    if wh == False:
        xmin1, ymin1, xmax1, ymax1 = box1
        xmin2, ymin2, xmax2, ymax2 = box2
    else:
        xmin1, ymin1 = int(box1[0]-box1[2]/2.0), int(box1[1]-box1[3]/2.0)
        xmax1, ymax1 = int(box1[0]+box1[2]/2.0), int(box1[1]+box1[3]/2.0)
        xmin2, ymin2 = int(box2[0]-box2[2]/2.0), int(box2[1]-box2[3]/2.0)
        xmax2, ymax2 = int(box2[0]+box2[2]/2.0), int(box2[1]+box2[3]/2.0)
    xx1 = np.max([xmin1, xmin2])
    yy1 = np.max([ymin1, ymin2])
    xx2 = np.min([xmax1, xmax2])
    yy2 = np.min([ymax1, ymax2])
    area1 = (xmax1-xmin1) * (ymax1-ymin1) 
    area2 = (xmax2-xmin2) * (ymax2-ymin2)
    inter_area = (np.max([0, xx2-xx1])) * (np.max([0, yy2-yy1]))
    iou = inter_area / (area1+area2-inter_area+1e-6)
    return iou
```

绘制模拟检测窗口

```python
from PIL import Image
import matplotlib.pyplot as plt

width, height = 600, 500
white_color = (255, 255, 255) 
image = Image.new("RGB", (width, height), white_color)
image.save("white_icon.png")

True_bbox, predict_bbox = [100, 35, 398, 400], [40, 150, 355, 398]

img = plt.imread('white_icon.png')
fig = plt.imshow(img)

fig.axes.add_patch(plt.Rectangle(
    xy=(True_bbox[0], True_bbox[1]), width=True_bbox[2]-True_bbox[0], height=True_bbox[3]-True_bbox[1],
    fill=False, edgecolor="blue", linewidth=2))
fig.axes.add_patch(plt.Rectangle(
    xy=(predict_bbox[0], predict_bbox[1]), width=predict_bbox[2]-predict_bbox[0], height=predict_bbox[3]-predict_bbox[1],
    fill=False, edgecolor="red", linewidth=2))
plt.show()
```

计算IOU值

```python
print(Iou(True_bbox, predict_bbox))
```

### MAP（Mean Average Precision）

目标检测问题中，每张图片都可能包含多个不同类别的物体，需要同时评估，物体分类和定位性能。在目标检测中，MAP是主要的衡量指标。MAP是多个分类任务的AP的平均值，而AP（average precision）是PR曲线下的面积。

检测结果分类：

1. True Positive（TP）：检测目标正确，同一个标记只计算一次，且$\text{IOU}\ge\text{IOU}_{\text{threshold}}=0.5$。
2. False Positive（FP）：
   1. $\text{IOU}<\text{IOU}_{\text{threshold}}=0.5$，检测框内没有任何目标，或者是只包含了目标很小的一部分。
   2. 检测到同一目标多余的检测框。
3. False Negative（FN）：没有检测到的目标数量。
4. True Negative（TN）：在MAP评价指标中不会使用到。

根据分类结构可以定义准确率和召回率的计算公式
$$
\text{Precision} 
=\frac{\text{TP}}{\text{TP}+\text{FP}}
=\frac{\text{TP}}{\text{All Detections}} \\
\text{Recall} 
= \frac{\text{TP}}{\text{TP} + \text{FN}}
=\frac{\text{TP}}{\text{All Ground Truths}}
$$
根据上述公式可以绘制PR曲线

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/1*KZu3UEBx3UIgOvdS6V_h_A.png" style="zoom:75%;" />

AP是计算某一类PR曲线下的面积，MAP则是计算所有类别PR曲线下面积的平均值。

假设有7张图片包含15个目标（绿色框），该15个目标属于同一类别（如：猫），以及24个预测结果（红色框），并且每个检测结果有一个置信度值。

![](https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/b29a8c2f80f50a4f4449f54206564a69.png)

根据图片顺序和检测结果，列出表格，IOU阈值为0.3

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/f4858db57f6ff6e37a8dc9eab3b6e3d7.png" style="zoom:70%;" />

根据置信度从大到小排序所有的预测框

![](https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/a716dde3f9ec8f40853db9783f6be744.png)

其中ACC TP和ACC FP是累积的TP和FP，Precision和Recall的值本质是衡量模型在所有已检测样本中的准确率和召回率，所以使用ACC TP和ACC FP计算。

1. 标号1

$$
\text{Precision} 
=\frac{\text{ACC TP}}{\text{ACC TP}+\text{ACC FP}}
=\frac{\text{1}}{\text{1+0}}
=1 \\
\text{Recall} 
=\frac{\text{ACC TP}}{\text{All Ground Truths}}
=\frac{\text{1}}{\text{15}}=0.0666
$$

2. 标号2

$$
\text{Precision} 
=\frac{\text{1}}{\text{1+1}}=0.5 \quad 
\text{Recall}
=\frac{\text{1}}{\text{15}}=0.0666
$$

3. 标号3

$$
\text{Precision} 
=\frac{\text{2}}{\text{2+1}}=0.6666 \quad 
\text{Recall}
=\frac{\text{2}}{\text{15}}=0.1333
$$

累积TP/FP能够动态反映随着检测数量增加，模型的精确率如何变化，例如：

1. 前10个检测结果中，若有8个TP和2个FP，此时的Precision为80%。
2. 若前100个检测结果中，累积到50个TP和50个FP，Precision降为50%。

累积TP/FP生成PR曲线

1. 按置信度从高到低遍历检测结果。
2. 每个检测结果被判定为TP或FP后，更新累积的TP和FP值。
3. 基于累积值计算当前阈值下的Precision和Recall，生成PR曲线。

累积TP/FP的统计方式：

* 能够动态反映模型在不同置信度阈值下的性能变化。
* 它符合Precision和Recall的数学定义（全局统计），而非局部统计。
* 它是生成PR曲线和计算MAP的基础。

绘制PR曲线

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/98e99336860ec11af4c5d3c5f7ee3888.png" style="zoom:55%;" />

AP值的计算：

1. 在VOC2010以前，选择Recall值在$[0,1]$区间，以0.1为步长，11个点准确率的平均值。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/205e875392effc827d67d9a971facdd6.png" style="zoom:50%;" />

​	计算结果如下
$$
\text{AP}=
\frac{1}{11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0)=26.84\%
$$
​	要计算MAP，就把所有类别的AP计算出来，然后求取平均即可。

2. 在VOC2010及以后，Recall值在$[0,1]$区间，以0.1为步长，选取其大于等于这些Recall值时的Precision最大值。

![](https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/map-p-r.jpg)

​	计算结果如下
$$
AP =A_1+A_2+A_3+A_4=24.55\%
$$
​	MAP值同样是所有类别的平均。

## 非极大值抑制（NMS）

非极大值抑制（Non-Maximum Suppression，NMS），顾名思义就是抑制不是极大值的元素。在目标检测中，NMS的目的就是要去除冗余的检测框，保留最好的一个。

NMS的处理流程

1. 检测出一系列的识别框。
2. 将检测框按照类别进行分类。
3. 对同一类别的检测框应用NMS获取最终的检测结果。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/bf487d66fb14f53db47cb1c722075ecd.png" style="zoom:50%;" />

NMS算法流程

1. 选择预测框列表X中，具有最大score的检测框$\alpha$，加入到最终的检测结果Y中，并从X中移除。
2. 计算X中剩余检测框，与$\alpha$的IOU值，$\text{IOU}(\beta_i, \alpha)>\text{IOU}_{\text{threshold}}=0.5$，将其从X中移除。
3. 对X中剩余检测框，重复这个过程，直到X为空。

算法示例

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/5ef5e6428d7a9ef4106476d5220293e7.png" style="zoom:65%;" />

1. 检测结果集X有A、B、C、D、E 5个候选框。
2. 因为B是得分最高的检测框，放入最终集Y，并将B从X中剔除。
3. 计算A、C、D、E与B的IOU。其中D、E的IOU大于0.5，从X中剔除D、E。
4. 剩下的检测框A和C，A得分最高，放入最终集Y，并将A从X中剔除。
5. 计算C与A的IOU，大于0.5，从X中剔除C。
6. 检测结果集X为空，算法结束。

使用程序实现NMS算法如下

```python
def nms(bboxes, confidence_score, threshold):
    if len(bboxes) == 0:
        return [], []
    
    bboxes = np.array(bboxes)
    score = np.array(confidence_score)

    x1 = bboxes[:, 0]
    y1 = bboxes[:, 1]
    x2 = bboxes[:, 2]
    y2 = bboxes[:, 3]

    picked_boxes = []
    picked_score = []

    order = np.argsort(score)
    areas = (x2 - x1) * (y2 - y1)
    while order.size > 0:
        index = order[-1]
        picked_boxes.append(bboxes[index])
        picked_score.append(confidence_score[index])
        x11 = np.maximum(x1[index], x1[order[:-1]])
        y11 = np.maximum(y1[index], y1[order[:-1]])
        x22 = np.minimum(x2[index], x2[order[:-1]])
        y22 = np.minimum(y2[index], y2[order[:-1]])
        w = np.maximum(0.0, x22 - x11)
        h = np.maximum(0.0, y22 - y11)
        intersection = w * h

        ratio = intersection / (areas[index] + areas[order[:-1]] - intersection)
        keep_boxes_indics = np.where(ratio < threshold)
        order = order[keep_boxes_indics] 
    return picked_boxes, picked_score

bounding = [(187, 82, 337, 317), (150, 67, 305, 282), (246, 121, 368, 304)]
confidence_score = [0.9, 0.65, 0.8]
threshold = 0.3
picked_boxes, picked_score = nms(bounding, confidence_score, threshold)
print('阈值threshold为:', threshold)
print('NMS后得到的bbox是：', picked_boxes)
print('NMS后得到的bbox的confidences是：', picked_score)
```

## 目标检测的速度

前传耗时（ms）：从输入一张图像到输出最终结果所消耗的时间，包括：前处理耗时（如图像归一化）、前向传播耗时、后处理耗时（如非极大值抑制）

每秒帧数（FPS）：每秒钟能处理的图像数量。

浮点运算量（FLOPS）：处理一张图像所需要的浮点运算数量，根具体硬件没有关系。可以公平地比较不同算法之间的检测速度。

## 目标检测方法分类

目标检测算法主要分为two-stage（两阶段）和one-stage（单阶段）两类：

* two-stage算法：先由算法生成一系列候选框，再通过卷积神经网络进行样本分类。主要通过一个卷积神经网络来完成目标检测过程，其提取的是CNN卷积特征，进行候选区域的筛选和目标检测两部分。网络的准确度高、速度相对较慢。代表算法是RCNN系列：从R-CNN到Faster R-CNN网络。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/7cf9af78c24e62d8470b5cbae89aae6e.png" style="zoom:65%;" />

* one-stage算法：直接通过主干网络给出目标的类别和位置信息，没有使用候选区域的筛选网路。这种算法速度快，但是精度相对two-stage网络有所降低。one-stage算法的代表是： YOLO系列算法、 SSD算法。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/cv/df6d54e022145d53fd96c023b1e2a33c.png" style="zoom:55%;" />
