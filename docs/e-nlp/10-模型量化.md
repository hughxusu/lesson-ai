# 模型量化

量化（Quantization）是一种通过降低模型参数的数值精度来减少计算资源占用、提升推理速度的技术，同时尽可能保持模型性能。其核心思想是用更少的比特（如4-bit、8-bit）表示原始模型中的高精度浮点数（如FP32），从而显著减少模型大小和内存需求。

模型的初始权重为FP32

* 如果将模型量化为FP16，则可以将模型大小减半。即需要一半的 GPU 显存即可加载量化后的模型。
* 如果将模型量化为8位整数，大约只需要四分之一的显存开销。
* 如果将模型量化为4位数据，大约只需要八分之一的显存开销。

模型参数计算

1. 参数总量：一般6.7B模型指一个含有大约6.7Billion（67亿）个参数的模型。
2. 一般模型默认使用FP16计算。
3. 显存总量

$$
\text{显存总量}
=\text{参数总量}\times\text{显存大小}
=67\text{亿}\times2\text{字节}
=13.4\times10^9\text{字节}
$$

4. 换算单位$1\text{GB}\approx 10^9$，模型显存大小约为13.4GB。

[Transformers中可以使用的量化方法](https://huggingface.co/docs/transformers/quantization/overview)

> [!warning]
>
> 从广义上讲模型量化，也是模型微调的一种形式。

## GPTQ

[GPTQ](https://arxiv.org/pdf/2210.17323)是一种后训练量化方法，它的核心思想是：在不需要重新训练模型的情况下，通过对量化误差的梯度估计，寻找尽可能减小量化误差的量化参数和权重表示，以尽量保持模型精度。

假设权重矩阵为$W$，输入特征为$X$，GPTQ计算的过程优化目标是找矩阵$\hat{W}$，其中
$$
Y=XW \qquad \hat{Y} = X\hat{W}
$$
优化的目标就是最小化输出误差
$$
\left\|Y-\hat{Y}\right\|_2^2=\left\|X\left(W-\hat{W}\right )\right\|_2^2
$$

### 算法思想

1. 计算协方差矩阵

$$
H=X^TX
$$

协方差矩阵，反映了不同维度之间的相关性，用于衡量量化误差对最终输出的真实影响。

2. 对于任意列$w_j$，对应的量化向量为$\hat{w_j}$，根据量化目标函数得

$$
\left\|Xw_j-X\hat{w}_j\right\|_2^2
$$

公式等价于
$$
\left(w_j-\hat{w_j}\right)^TH\left(w_j-\hat{w}_j\right)
$$

3. 量化后第$j$，误差为

$$
e_j=w_j-\hat{w}_j
$$

误差在后续量化其他列时会传播，使用消元法纠正量化误差
$$
W_{\text{remain}}\gets W_{\text{remain}}-\frac{e_j}{H_{jj}}H_{j,\text{remain}}
$$

4. 重复上面过程，直到所有列量化。

> [!warning]
>
> GPTQ只能优化线性层，只要层运算可以表示为矩阵乘法$Y=XW$，即可用用改方法优化。可转化为矩阵乘法的卷积层，注意力机制中的线性部分。

### 算法流程

1. 将样本数据送入模型计算，记录各层的输入张量$x$。
2. 对每个每个线性层，取权重矩阵 $W$，按行（或分块）处理。
3. 对于每一行权重$w_j$，根据误差传播进行量化。
4. 重复算法，直到所有列量化。

### 实践案例

安装量化库

```shell
pip install auto-gptq optimum
```

* **`auto-gptq`**：核心库，提供量化实现和API
* **`optimum`**：Hugging Face的优化库，封装GPTQ接口，简化量化流程

设置模型id

```python
model_id = "facebook/opt-2.7b"
```

设置量化参数

```python
from transformers import GPTQConfig

quantization_config = GPTQConfig(
     bits=4, group_size=128, dataset="wikitext2", desc_act=False,
)
```

* `bits=4`量化精度，支持的精度有`[2, 4, 6, 8]`
* `group_size=128`量化批次的数据，超参数。
* `dataset`使用量化的数据集，包括`['wikitext2','c4','c4-new','ptb','ptb-new']`。可以传入自定义数据集。

模型量化

```python
from transformers import AutoModelForCausalLM

quantized_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    device_map='auto'
)
```

* `AutoModelForCausalLM`因果语言模型，即根据上下文生成后续文本的任务。

保存量化结果

```python
save_path = "./data/hf/models/quantized-opt-2.7b" 
quantized_model.save_pretrained(save_path)
```

读取量化模型

```python
loaded_model = AutoModelForCausalLM.from_pretrained(
    save_path, device_map='auto', torch_dtype='auto'
)
```

读取配套分词器

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_id)
```

测试量化后的结果

```python
text = "Python is the best programming language."
inputs = tokenizer(text, return_tensors="pt").to(0)

out = loaded_model.generate(**inputs, max_new_tokens=64)
print(tokenizer.decode(out[0], skip_special_tokens=True))
```

未量化模型生成内容

```python
from transformers import pipeline

generator = pipeline(
    'text-generation', model=model_id, device=0, do_sample=True, num_return_sequences=3
)
print(*generator(text), sep='\n')
