# 模型量化

量化（Quantization）是一种通过降低模型参数的数值精度来减少计算资源占用、提升推理速度的技术，同时尽可能保持模型性能。其核心思想是用更少的比特（如4-bit、8-bit）表示原始模型中的高精度浮点数（如FP32），从而显著减少模型大小和内存需求。

模型的初始权重为FP32

* 如果将模型量化为FP16，则可以将模型大小减半。即需要一半的 GPU 显存即可加载量化后的模型。
* 如果将模型量化为8位整数，大约只需要四分之一的显存开销。
* 如果将模型量化为4位数据，大约只需要八分之一的显存开销。

模型参数计算

1. 参数总量：一般6.7B模型指一个含有大约6.7Billion（67亿）个参数的模型。
2. 一般模型默认使用FP16计算。
3. 显存总量

$$
\text{显存总量}
=\text{参数总量}\times\text{显存大小}
=67\text{亿}\times2\text{字节}
=13.4\times10^9\text{字节}
$$

4. 换算单位$1\text{GB}\approx 10^9$，模型显存大小约为13.4GB。

[Transformers中可以使用的量化方法](https://huggingface.co/docs/transformers/quantization/overview)

> [!warning]
>
> 从广义上讲模型量化，也是模型微调的一种形式。

## GPTQ

[GPTQ](https://arxiv.org/pdf/2210.17323)是一种后训练量化方法，它的核心思想是：在不需要重新训练模型的情况下，通过对量化误差的梯度估计，寻找尽可能减小量化误差的量化参数和权重表示，以尽量保持模型精度。

假设权重矩阵为$W$，输入特征为$X$，GPTQ计算的过程优化目标是找矩阵$\hat{W}$，其中
$$
Y=XW \qquad \hat{Y} = X\hat{W}
$$
优化的目标就是最小化输出误差
$$
\left\|Y-\hat{Y}\right\|_2^2=\left\|X\left(W-\hat{W}\right )\right\|_2^2
$$

### 算法思想

1. 计算协方差矩阵

$$
H=X^TX
$$

协方差矩阵，反映了不同维度之间的相关性，用于衡量量化误差对最终输出的真实影响。

2. 对于任意列$w_j$，对应的量化向量为$\hat{w_j}$，根据量化目标函数得

$$
\left\|Xw_j-X\hat{w}_j\right\|_2^2
$$

公式等价于
$$
\left(w_j-\hat{w_j}\right)^TH\left(w_j-\hat{w}_j\right)
$$

3. 量化后第$j$，误差为

$$
e_j=w_j-\hat{w}_j
$$

误差在后续量化其他列时会传播，使用消元法纠正量化误差
$$
W_{\text{remain}}\gets W_{\text{remain}}-\frac{e_j}{H_{jj}}H_{j,\text{remain}}
$$

4. 重复上面过程，直到所有列量化。

> [!warning]
>
> GPTQ只能优化线性层，只要层运算可以表示为矩阵乘法$Y=XW$，即可用用改方法优化。可转化为矩阵乘法的卷积层，注意力机制中的线性部分。

### 算法流程

1. 将样本数据送入模型计算，记录各层的输入张量$x$。
2. 对每个每个线性层，取权重矩阵 $W$，按行（或分块）处理。
3. 对于每一行权重$w_j$，根据误差传播进行量化。
4. 重复算法，直到所有列量化。

假设权重矩阵W为形状为`[10, 5]`，如下:

```
    输出神经元 (5个)
    ↓
[ w00, w01, w02, w03, w04 ]  ← 输入特征0 连接到 所有5个输出
[ w10, w11, w12, w13, w14 ]  ← 输入特征1 连接到 所有5个输出
[ w20, w21, w22, w23, w24 ]  ← 输入特征2 连接到 所有5个输出
[ w30, w31, w32, w33, w34 ]  ← ...
[ w40, w41, w42, w43, w44 ]
[ w50, w51, w52, w53, w54 ]
[ w60, w61, w62, w63, w64 ]
[ w70, w71, w72, w73, w74 ]
[ w80, w81, w82, w83, w84 ]
[ w90, w91, w92, w93, w94 ]  ← 输入特征9 连接到 所有5个输出
```

其量化过程：

1. 量化第0列；
2. 量化第1列；
3. 以此类推，完成全部参数量化。

误差计算

1. 确定缩放因子（scale）和零点（zero_point）：
   * `scale = 0.1`
   * `zero_point = 0`
2. 量化计算

$$
\text{round}\left(\frac{0.73}{0.1}\right)=\text{round}\left(7.3\right)=7
$$

3. 量化后的整数7，用于存储。但进行前向传播计算时，会被临时恢复为浮点数。

$$
\text{dequantized}=7\times0.1=0.7
$$

4. 计算误差

$$
e_0=0.7-0.73=-0.03
$$

量化公式
$$
W_{\text{remain}}\gets W_{\text{remain}}-\frac{e_j}{H_{jj}}H_{j,\text{remain}}
$$

* 根据列元素的范围 $j\in[0, 9]$
* $W_{\text{remain}}$（向量）：表示这一列数据余下的部分。
* $e_j$（标量）：当前正在处理的第 $j$ 个权重元素的量化误差。
* $H_{jj}$（标量）：Hessian矩阵第 $j$ 行第 $j$ 列的对角线元素。它代表了第 $j$ 个输入特征自身的重要性。
* $H_{j, remain}$ （向量）：Hessian矩阵第 $j$  行中，与所有尚未量化的权重元素对应的那些列所组成的向量。它编码了第 $j$ 个输入特征与所有“剩余”输入特征之间的相关性。

### 实践案例

安装量化库

```shell
pip install auto-gptq optimum
```

* **`auto-gptq`**：核心库，提供量化实现和API
* **`optimum`**：Hugging Face的优化库，封装GPTQ接口，简化量化流程

设置模型id

```python
model_id = "facebook/opt-2.7b"
```

设置量化参数

```python
from transformers import GPTQConfig

quantization_config = GPTQConfig(
     bits=4, group_size=128, dataset="wikitext2", desc_act=False,
)
```

* `bits=4`量化精度，支持的精度有`[2, 4, 6, 8]`
* `group_size=128`量化批次的数据，超参数。
* `dataset`使用量化的数据集，包括`['wikitext2','c4','c4-new','ptb','ptb-new']`。可以传入自定义数据集。

模型量化

```python
from transformers import AutoModelForCausalLM

quantized_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    device_map='auto'
)
```

* `AutoModelForCausalLM`因果语言模型，即根据上下文生成后续文本的任务。

保存量化结果

```python
save_path = "./data/hf/models/quantized-opt-2.7b" 
quantized_model.save_pretrained(save_path)
```

读取量化模型

```python
loaded_model = AutoModelForCausalLM.from_pretrained(
    save_path, device_map='auto', torch_dtype='auto'
)
```

读取配套分词器

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_id)
```

测试量化后的结果

```python
text = "Python is the best programming language."
inputs = tokenizer(text, return_tensors="pt").to(0)

out = loaded_model.generate(**inputs, max_new_tokens=64)
print(tokenizer.decode(out[0], skip_special_tokens=True))
```

未量化模型生成内容

```python
from transformers import pipeline

generator = pipeline(
    'text-generation', model=model_id, device=0, do_sample=True, num_return_sequences=3
)
print(*generator(text), sep='\n')
