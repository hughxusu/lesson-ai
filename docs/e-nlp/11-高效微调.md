# 大模型高效微调

参数高效微调（PEFT，Parameter-Efficient Fine-Tuning）是一种针对大规模预训练模型进行微调的技术，旨在通过仅调整少量参数或引入少量额外参数，去适应新的任务。

PEFT的优点：

1. 节省计算资源和存储：传统微调要保存一份完整模型的参数，而PEFT只需要保存少量新增或修改的参数，节省显著的存储空间。
2. 微调速度更快：调整参数更少，训练时计算量和时间都降低，适合资源有限的环境。
3. 保持预训练知识：冻结大部分预训练模型的权重，防止灾难性遗忘。
4. 适用于低资源场景：可在消费级GPU、边缘设备（如手机、IoT设备）上运行，降低训练和部署成本。

## PEFT主要方法

### Adapter Tuning

在Transformer层中插入小型神经网络模块（Adapter），仅训练这些模块。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nlp/Adapter-Tuning.png" style="zoom:95%;" />

1. 模型预训练阶段不加入Adapter。
2. 经典的Adapter就是全连接网络。
3. 在模型微调阶段，根据下游任务的数据训练Adapter，预训练中模型的参数都被冻结。

相关论文：[Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751)

优点

1. 参数高效，仅微调少量参数（通常占主模型的0.1%~1%）。
2. 冻结主模型参数，避免灾难性遗忘，保留预训练学到的通用知识。
3. 多任务适配，同一预训练模型可服务多个任务，只需切换不同的 Adapter。
4. 训练速度快，相比全参数微调，Adapter Tuning的训练时间显著缩短。

缺点

1. 每层增加2个额外矩阵乘法，引入推理延迟。
2. 达能力受限，Adapter限制了其表达能力，可能无法完全适配复杂任务。

### Prefix Tuning

在输入序列前添加一组可学习的“前缀向量”（Prefix），通过调整这些向量来引导模型生成符合任务要求的输出，而无需微调整个模型参数。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nlp/An-illustration-of-prefix-tuning.ppm.png" style="zoom:75%;" />

1. 模型预训练阶段不加入Prefix。
2. Prefix有两种添加方式：
   1. 无投影的Prefix，将Prefix直接拼接到Key和Value上，同时扩展Key和Value的矩阵。
   2. 带投影的Prefix，就是在key和value之前再增加一层全连接矩阵。
3. 在模型微调阶段，训练Prefix参数，预训练中模型的参数都被冻结。

优点

1. 参数高效，仅微调少量参数（通常占主模型的0.1%~1%）。
2. 冻结主模型参数，避免灾难性遗忘，保留预训练学到的通用知识。
3. 训练速度快，相比全参数微调，训练时间显著缩短。
4. 直接控制注意力机制，适合文本生成、对话等任务。

缺点

1. 在非生成任务（如分类）上效果可能不如Adapter。
2. Prefix过长会浪费计算，过短可能表达能力不足。
3. 随机初始化可能导致训练不稳定。

相关论文：[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190)

### Prompt Tuning

通过优化连续的软提示（Soft Prompts）来激活模型内部知识，从而适配下游任务，而无需微调整个模型参数。

![](https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nlp/PEPT-Model-Structure-Lester-et-al-2021.jpg)

硬提示（Hard Prompt）：是指以自然语言形式人工设计的提示词，即传统意义上的文本指令或问题模板。

软提示（Soft Prompts：虚拟的token与真实的词表无关，它们本质上是一组独立于词表的可训练参数，通过反向传播学习最优数值，无需对应任何自然语言词汇。

1. 原始输入文本（如句子或问题）通过预训练模型的词嵌入层转换为向量序列。
2. 定义一组可训练的连续向量（软提示），这些向量不是自然语言，而是通过梯度下降优化的高维数值。
3. 将软提示直接拼接到输入向量前。
4. 训练过程中，冻结预训练模型的所有参数，仅优化提示向量。
5. 可以通过计算相似度在词表中查找与软提示向量相近的真实token，但结果通常无实用语义。

优点

1. 参数高效，仅微调少量参数（通常占主模型的0.01%~0.1%）。
2. 冻结主模型参数，避免灾难性遗忘，保留预训练学到的通用知识。
3. 训练速度快，相比全参数微调，训练时间显著缩短。
4. 多任务友好，同一模型只需切换不同软提示即可适配新任务。

缺点

1. 小数据敏感，在极小数据集（如 <100 样本）上可能表现不稳定。
2. 表达能力有限。
3. 提示长度影响大，增加软提示的长度，会导致输入信息下降。

相关论文：[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691)

### LoRA

LoRA（Low-Rank Adaptation）通过低秩矩阵分解模拟全参数微调的效果，仅训练少量参数即可适配下游任务。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nlp/56ccd6fb679.jpeg" style="zoom:55%;" />

预训练模型的权重矩阵$W$，在微调时的变化集中在少数维度上，表示为$\Delta W$，微调后原始权重的前向传播$W$表示为
$$
h=Wx+\Delta Wx
$$
而在微调中$\Delta W$具有低秩特性（即变化集中在少数维度上），所以可以将权重更新分解为两个小矩阵的乘积
$$
\Delta W = A\cdot B
$$
其中$A$和$B$的秩（rank）通常取 1~64（远小于原矩阵维度），所以前向传播可以表示为
$$
h=Wx+(A\cdot B)x
$$
LoRA的$\Delta W$以旁路形式作用于原始权重$W$，与残差网络的残差连接有相似之处。

LoRA在Transformer中的常见添加位置

1. Query和Value矩阵（Key矩阵通常不添加$\Delta W$）。
2. 部分变体会应用于FFN的中间层矩阵。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nlp/lora.png" style="zoom:55%;" />

优点：

1. 参数高效，仅微调少量参数（通常占主模型的0.1%~1%）。
2. 冻结主模型参数，避免灾难性遗忘，保留预训练学到的通用知识。
3. 无推理延迟，合并 $W+AB$后，推理速度与原始模型一致。
4. 多任务适配，同一预训练模型可服务多个任务，只需切换不同的$A$和$B$矩阵 。

缺点

1. 表达能力有限。
2. 超参数敏感，秩（rank）设置不当可能导致训练不稳定或收敛困难。
3. 随着秩增大，LoRA 可能引入与任务无关的噪声幻觉。

相关论文：[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685)

### LoRA的其它变体

AdaLoRA是LoRA的改进版本，核心思想是动态调整低秩矩阵的秩，自动分配不同层的秩，重要层分配更高秩，次要层降低秩甚至关闭更新。其中采用了复杂的剪枝策略。

相关论文：[AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.10512)

QLoRA是LoRA的量化升级版，过4-bit量化大幅降低显存占用，使微调超大规模参数成为可能，其性能接近全参数微调。

* 主模型权重$W$被4-bit量化，推理时动态反量化。
* LoRA适配器$\Delta W$不量化，始终保持16-bit（BF16/FP16）精度。

相关论文：[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314)

## PEFT实践

[Hugging Face PEFT库](https://huggingface.co/docs/peft/index)是一个为大型预训练模型提供多种高效微调方法的Python库，其中包括了Adapter Tuning、Prefix Tuning、Prompt Tuning和LoRA等方法。安装PEFT库

```shell
pip install peft
```

### 微调实践入门

以[facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b)模型为例进行模型微调

```python
from transformers import GPT2Tokenizer, OPTForCausalLM

model_id = "facebook/opt-6.7b"

model = OPTForCausalLM.from_pretrained(model_id, load_in_8bit=True)
tokenizer = GPT2Tokenizer.from_pretrained(model_id)
```

使用原始模型生成内容

```python
text = "Python is the best programming language."

inputs = tokenizer(text, return_tensors="pt").to(0)  
outputs = model.generate(**inputs, max_length=50)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Generated Text:", generated_text)
```

模型预处理

* 将所有非`int8`模块转换为全精度FP32以保证稳定性
* 为输入嵌入层添加一个`forward_hook`，以启用输入隐藏状态的梯度计算
* 启用梯度检查点以实现更高效的内存训练

```python
from peft import prepare_model_for_int8_training

model = prepare_model_for_int8_training(model)
```

查看显存占用和模型结构

```python
memory_footprint_bytes = model.get_memory_footprint()
memory_footprint_mib = memory_footprint_bytes / (1024 ** 3)  # 转换为 GB

print(f"{memory_footprint_mib:.2f}GB")
print(model)
```

配置LoRA参数

```python
from peft import LoraConfig, get_peft_model

# 创建一个LoraConfig对象，用于设置LoRA的配置参数
config = LoraConfig(
    r=8,            # LoRA的秩，影响LoRA矩阵的大小
    lora_alpha=32,  # LoRA适应的比例因子
    # 指定将LoRA应用到的模型模块，通常是attention和全连接层的投影
    target_modules = ["q_proj", "k_proj", "v_proj", "out_proj", "fc_in", "fc_out"],
    lora_dropout=0.05,     # 在LoRA模块中使用的dropout率
    bias="none",           # 设置bias的使用方式，这里没有使用bias
    task_type="CAUSAL_LM"  # 任务类型，这里设置为因果(自回归）语言模型
)

# 使用get_peft_model函数和给定的配置来获取一个PEFT模型
model = get_peft_model(model, config)
model.print_trainable_parameters()
print(model)
```

使用[Abirate/english_quotes](https://huggingface.co/datasets/Abirate/english_quotes)数据集进行模型微调

```python
from datasets import load_dataset

dataset = load_dataset("Abirate/english_quotes")
print(dataset["train"])
```

打印数据格式

```python
from datasets import ClassLabel, Sequence
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):
            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])
    display(HTML(df.to_html()))
    
show_random_elements(dataset["train"])
```

将分词转换为训练数据

```python
from transformers import DataCollatorForLanguageModeling

tokenized_dataset = dataset.map(lambda samples: tokenizer(samples["quote"]), batched=True)
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

* `data_collator`数据整理器，训练过程中动态生成批数据，`mlm=False`模型的任务是预测下一个token。

设置训练参数

```python
from transformers import TrainingArguments, Trainer

save_path = "./data/hf/models/opt-6.7b-lora"

training_args = TrainingArguments(
    output_dir=save_path,           # 指定模型输出和保存的目录
    per_device_train_batch_size=4,  # 每个设备上的训练批量大小
    learning_rate=2e-4,             # 学习率
    fp16=True,                      # 启用混合精度训练，可以提高训练速度，同时减少内存使用
    logging_steps=20,               # 指定日志记录的步长，用于跟踪训练进度
    max_steps=100,                  # 最大训练步长
    num_train_epochs=1              # 训练的总轮数
)
```

配置训练器

```python
trainer = Trainer(
    model=model,                               # 指定训练时使用的模型
    train_dataset=tokenized_dataset["train"],  # 指定训练数据集
    args=training_args,
    data_collator=data_collator,
)

model.use_cache = False # 禁用模型的自回归生成缓存
```

训练模型

```python
trainer.train()
```

保存训练结果

```python
model.save_pretrained(save_path)
```

测试训练结果

```python
lora_model = trainer.model

inputs = tokenizer(text, return_tensors="pt").to(0)

out = lora_model.generate(**inputs, max_length=50)
print(tokenizer.decode(out[0], skip_special_tokens=True))
```



