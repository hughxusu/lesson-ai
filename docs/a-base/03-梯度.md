# 梯度下降法

梯度下降法（Gradient Descent）是一种，用于优化目标函数的**迭代算法**。

* 是一种基于搜索的最优化方法。
* 最小化损失函数（最大化效用函使用梯度上升法）。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/nn/resizem_fixedw_1184.png" style="zoom:35%;" />

对于简单线性回归来说，损失函数如下：
$$
\sum_{i=1}^m \left ( y^{(i)}-ax^{(i)}-b \right )^2
$$
函数中$a$和$b$是变量，由于$b$是一个偏移量，考虑与特征相关的变量只有$a$，所以损失函数可以表示为
$$
L=f(\theta)
$$
对于只有一个参数的损失函数图像如下

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/8b3433626c22d7f31a236492458a58e0.jpg" style="zoom:90%;" />

纵坐标表示损失函数$L$的值，横坐标表示系数$\theta$。每一个$\theta$值都会对应一个损失函数$L$的值，使损失函数$L$​最小，就是找到曲线的最小值点。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/7d33b5dbd153029338804e42063e7dae.jpg" style="zoom:90%;" />

对于曲线上任意一点A，直线M是点A的切线，A的导数就是直线M的斜率，切线的斜率即为导数的符号。而导数的正负，可以指出损失函数$L$增加的方向。

* 如果 $\frac{\mathrm{d} L}{\mathrm{d} \theta } >0 $，沿横坐标正方向上损失函数值增大。
* 如果 $\frac{\mathrm{d} L}{\mathrm{d} \theta } <0 $，沿横坐标负方向上损失函数值增大。

对于任意一点A的取值其横坐标为$\theta_1$，对应的损失函数值为$L(\theta_1)$​，对改点的损失函数值，加一个导数值，则有
$$
\theta_1 + {L}' (\theta_1)
$$
可以看出，增加一个导数值后，损失函数是在增加的，如果希望损失函数值不断变小，需要在$\theta_1$的基础上减小一个导数值，则有
$$
\theta_1 - \eta  {L}' (\theta_1)
$$
上式在移动过程中增加了一个比例参数$\eta$，可以更好的控制移动的幅度，

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/79181d4455f5a40c4a652a41a743bc7b.jpg" style="zoom:90%;" />

$\theta_1$减小后移动到$\theta_2$的位置，可以看出损失函数在减小，但是并没有达到极值点，因为B点的导数也不为0。在$\theta_2$的基础上继续减小系数，可以得到

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/12d864def3bdafa3f3ecda683f2950e4.jpg" style="zoom:90%;" />

通过迭代可以找到损失函数的极小值，在上面的曲线中，极小值就是最小值。

$\eta$在机器学习中称为学习率：

* $\eta$​的取值影响获得最优解的速度。
* $\eta$​取值不合适时甚至得不到最优解。
* $\eta$是梯度下降法的一个超参数。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/eta-normal.png" style="zoom: 33%;" />

$\eta$过小学习的收敛速度慢。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/eta-small.png" style="zoom:34%;" />

$\eta$​过大导致不收敛。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/eta-large.png" style="zoom:34%;" />

不是所有函数都有唯一的极值点，所有梯度下线法收敛的点不一定都是最小值点。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/global-local.png" style="zoom:55%;" />

解决方法时，随机初始化起始点，多次运行。梯度下降法的初始点也是一个超参数。对于简单线性回归的损失函数具有唯一的最优解。

## 模拟梯度下降法

模拟损失函数曲线

```python
import numpy as np
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))
plot_x = np.linspace(-1, 6, 141)
plot_y = (plot_x - 2.5) ** 2 - 1

plt.plot(plot_x, plot_y)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.show()
```

使用梯度下降法求最小值，绘制求解过程中的$\theta$取值。

```python
def dJ(theta):
    return 2 * (theta - 2.5)

def J(theta):
    return (theta - 2.5) ** 2 - 1

theta_history = []
def gradient_descent(initial_theta, eta, epsilon=1e-8):
    theta = initial_theta
    theta_history.append(initial_theta)
    while True:
        gradient = dJ(theta)
        last_theta = theta
        theta = theta - eta * gradient
        theta_history.append(theta)
        if abs(J(theta) - J(last_theta)) < epsilon:
            break
    return theta

def plot_theta_history():
    plt.figure(figsize=(10, 8))
    plt.plot(plot_x, J(plot_x))
    plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='+')
    plt.xticks(fontsize=16)
    plt.yticks(fontsize=16)
    plt.show()
```

当$\eta$变小时

```python
eta = 0.01
theta_history = []
gradient_descent(0., eta)
plot_theta_history()
```

当$\eta$变大时

```python
eta = 0.8
theta_history = []
gradient_descent(0., eta)
plot_theta_history()
```

> [!warning]
>
> $\eta$的取值与求导结果有关，$\eta$可以采用网格搜索的方法，经验值一般取0.01

## 线性回归中的梯度下降法

对于一般的线性回归公式
$$
\theta=\left( \theta_0, \theta_1, \cdots,\theta_n \right)
$$
梯度下降的表达式为
$$
-\eta\nabla L \qquad
\nabla L = \left( \frac{\partial L}{\partial \theta_0}   , \frac{\partial L}{\partial \theta_1} , \cdots,\frac{\partial L}{\partial \theta_n}  \right)
$$
其中$\nabla L$称为梯度值。在高维空间中，使用梯度代替导数，来移动$\theta$的值以便找到最小值点。对于有两个参数的梯度下降法示意图如下

![](https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/2d-g.png)

> [!warning]
>
> 梯度的几何意义是函数在该点增长最快的方向，其反方向是下降最快的方向。这是梯度下降法，将梯度作为$\theta$下降方向的原因。

损失函数为
$$
\sum_{i=1}^m \left ( y^{(i)}-\hat{y}^{(i)} \right )^2
$$
对于高维向量其中
$$
\hat{y}^{(i)}=\theta_0+\theta_1x_1^{(i)}+\theta_2x_2^{(i)}+\cdots+\theta_nx_n^{(i)}
$$
所有目标函数转换为
$$
\sum_{i=1}^m \left ( y^{(i)}-\theta_0-\theta_1x_1^{(i)}-\theta_2x_2^{(i)}-\cdots-\theta_nx_n^{(i)} \right )^2
$$
上式就是损失函数$L$，其中
$$
\hat{y}^{(i)} =X^{(i)}_b \cdot \theta
$$
所以$\nabla L$，可以表示为
$$
\nabla L(\theta) = \begin{pmatrix}
\frac{\partial L}{\partial \theta_0 } \\
\frac{\partial L}{\partial \theta_1 } \\
\frac{\partial L}{\partial \theta_2 } \\
\cdots \\
\frac{\partial L}{\partial \theta_n } 
\end{pmatrix} = \begin{pmatrix}
\sum_{i=1}^m 2\left ( y^{(i)}-X_b^{(i)}\theta \right )(-1) \\
\sum_{i=1}^m 2\left ( y^{(i)}-X_b^{(i)}\theta \right )(-x_1^{(i)}) \\
\sum_{i=1}^m 2\left ( y^{(i)}-X_b^{(i)}\theta \right )(-x_2^{(i)}) \\
\cdots \\
\sum_{i=1}^m 2\left ( y^{(i)}-X_b^{(i)}\theta \right )(-x_n^{(i)})

\end{pmatrix}=2\begin{pmatrix}
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right ) \\
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right )x_1^{(i)} \\
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right )x_2^{(i)} \\
\cdots \\
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right )x_n^{(i)}

\end{pmatrix}
$$
上式中梯度计算与$m$相关，为了使梯度计算与$m$无关，则目标函数可以转换为
$$
\frac{1}{m}\sum_{i=1}^m \left ( y^{(i)}-\hat{y}^{(i)} \right )^2
$$
所以梯度计算可以表示为
$$
\nabla L(\theta) = \begin{pmatrix}
\frac{\partial L}{\partial \theta_0 } \\
\frac{\partial L}{\partial \theta_1 } \\
\frac{\partial L}{\partial \theta_2 } \\
\cdots \\
\frac{\partial L}{\partial \theta_n } 
\end{pmatrix} =\frac{2}{m}\begin{pmatrix}
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right ) \\
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right )x_1^{(i)} \\
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right )x_2^{(i)} \\
\cdots \\
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right )x_n^{(i)}

\end{pmatrix}
$$
所以目标函数可以定义为
$$
L(\theta)=MSE(y,\hat{y})
$$

### 使用梯度计算线性回归

汽车销售数据如下

```python
x = [9, 9.23, 9.41, 9.68, 9.98, 10.20, 10.58, 10.68, 10.88, 10.98, 11.28, 11.38, 11.56, 11.88, 12.00]
y = [12.23, 11.7, 10.21, 9.60, 8.72, 7.70, 7.10, 6.61, 6.10, 5.82, 5.50, 5.23, 4.65, 4.20, 3.50]

x = np.array(x)
y = np.array(y)

plt.figure(figsize=(10, 8))
plt.scatter(x, y, s=120)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.show()
```

使用梯度下降算法训练参数

```python
def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
    except:
        return float('inf')
    
def dJ(theta, X_b, y):
    res = np.empty(len(theta))
    res[0] = np.sum(X_b.dot(theta) - y)
    for i in range(1, len(theta)):
        res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
    return res * 2 / len(X_b)

def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e8, epsilon=1e-8):
    theta = initial_theta
    cur_iter = 0
    while cur_iter < n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon:
            break
        cur_iter += 1
    return theta
```

使用梯度下降法，计算参数

```python
X = x.reshape(-1, 1)

X_b = np.hstack([np.ones((len(X), 1)), X])
initial_theta = np.zeros(X_b.shape[1])
eta = 0.007

theta = gradient_descent(X_b, y, initial_theta, eta)
print(theta)
```

梯度下降的计算过程

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/0716617.gif" style="zoom:70%;" />

### 向量化计算

对于导数的计算
$$
\nabla L(\theta) = \frac{2}{m}\begin{pmatrix}
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right ) \\
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right )x_1^{(i)} \\
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right )x_2^{(i)} \\
\cdots \\
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right )x_n^{(i)}
\end{pmatrix} = \frac{2}{m}\begin{pmatrix}
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right )x_0^{(i)} \\
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right )x_1^{(i)} \\
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right )x_2^{(i)} \\
\cdots \\
\sum_{i=1}^m \left ( X_b^{(i)}\theta - y^{(i)} \right )x_n^{(i)}
\end{pmatrix} \tag{1}
$$
其中$x_0^{(i)}\equiv 1 $，所以上式可以整理成
$$
\nabla L(\theta) = \frac{2}{m}\left( X_b^{(1)}\theta - y^{(1)}, X_b^{(2)}\theta - y^{(2)}, \cdots, X_b^{(m)}\theta - y^{(m)} \right) \cdot  \begin{pmatrix}
x_0^{(1)}, x_1^{(1)}, \cdots ,x_n^{(1)} \\
x_0^{(2)}, x_1^{(2)}, \cdots ,x_n^{(2)} \\
\cdots \\
x_0^{(m)}, x_1^{(m)}, \cdots ,x_n^{(m)} \\
\end{pmatrix}
$$
后面的矩阵就是$X_b$，所以上式还可以写为
$$
\nabla L(\theta) = \frac{2}{m}\left( X_b \theta -y \right)^T \cdot X_b
$$
由于上述算式是一个行向量，所以为了保证计算结果是列向量，使用如下算式
$$
\frac{2}{m}X_b^T \cdot \left( X_b \theta -y \right)
$$
上面的梯度计算代码简化并封装如下：

```python
def fit_gd(X_train, y_train, eta=0.01, n_iters=1e4):
    assert X_train.shape[0] == y_train.shape[0], 'the size of X_train must be equal to the size of y_train'
    
    def J(theta, X_b, y):
        try:
            return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
        except:
            return float('inf')
        
    def dJ(theta, X_b, y):
        return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(y)
    
    def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
        theta = initial_theta
        cur_iter = 0
        while cur_iter < n_iters:
            gradient = dJ(theta, X_b, y)
            last_theta = theta
            theta = theta - eta * gradient
            if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon:
                break
            cur_iter += 1
        return theta
    
    X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
    initial_theta = np.zeros(X_b.shape[1])
    theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters)
    return theta
```

使用梯度下降法计算参数

```python
theta = fit_gd(X, y, eta=0.007)
print(theta)
```

## 随机梯度下降法

计算梯度的过程中，对于公式 $(1)$ 来说是将全部 $m$ 个样本，用于梯度计算。假设计算过程中每次只取一个样本则有
$$
\nabla L(\theta) = \frac{2}{m}\begin{pmatrix}
\left ( X_b^{(i)}\theta - y^{(i)} \right )X_0^{(i)} \\
\left ( X_b^{(i)}\theta - y^{(i)} \right )X_1^{(i)} \\
\left ( X_b^{(i)}\theta - y^{(i)} \right )X_2^{(i)} \\
… \\
\left ( X_b^{(i)}\theta - y^{(i)} \right )X_n^{(i)}
\end{pmatrix} \tag{1} = \frac{2}{m}\left( X_b^{(i)} \theta -y \right)^TX_b^{(i)}
$$
如果每次迭代随机选择一个样本用于最小化迭代，则称为随机梯度下降发。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/92e28ad8c4fa25fd273d2d0d92c7a1ec.jpg" style="zoom:45%;" />

随机梯度下降算法如下

```python
def fit_sgd(X_train, y_train, eta=0.01, n_iters=10000):
    assert X_train.shape[0] == y_train.shape[0], 'the size of X_train must be equal to the size of y_train'

    def dJ_sgd(theta, x_b, y):
        return 2 * x_b.T.dot(x_b.dot(theta) - y)

    def sgd(X_b, y, initial_theta, eta, n_iters=50):
        theta = initial_theta
        m = len(X_b)
        for _ in range(n_iters):
            indices = np.random.permutation(m)
            X_b_shuffled = X_b[indices]
            y_shuffled = y[indices]
            for i in range(m):
                xi = X_b_shuffled[i:i+1]  # shape (1, n)
                yi = y_shuffled[i]
                gradient = dJ_sgd(theta, xi, yi)
                theta = theta - eta * gradient
        return theta

    X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
    initial_theta = np.zeros(X_b.shape[1])
    theta = sgd(X_b, y_train, initial_theta, eta, n_iters)
    return theta
```

* 使用`np.random.permutation(m)`将数据打乱顺序，用来代替随机抽样过程。
* 每个`epoch`表示将所以数据迭代一遍。

测试随机梯度下降结果

```python
theta = fit_sgd(X, y, eta=0.007)
print(theta)
```

## 特征归一化

判断肿瘤是良性还是恶性

|       | 肿瘤大小（厘米） | 发现时间（天） | 发现时间（年） |
| ----- | ---------------- | -------------- | -------------- |
| 样本1 | 1                | 200            | 0.55年         |
| 样本2 | 5                | 100            | 0.27年         |

数据不同维度的量纲不同，会直接影响数据距离的计算。

1. 当发现时间的单位为天时，样本间的距离被发现时间所主导。
2. 当发现时间的单位为年时，样本间的距离被肿瘤大小所主导。

数据归一化是将所有不同量纲的数据，映射在一个尺度下。

> [!warning]
>
> 理论上可以证明，归一化数据不影响分类结果，但可以加快学习速率。

### 最值归一化

把所有的数据映射到0~1之间
$$
x_{\text{sacle}}=\frac{x-x_{\min}}{x_{\max}-x_{\min}}
$$
适用于分布有明显边界特征，受异常值影响比较大，如：数据集 $1,2,3, 1000, …$

* 学生考试成绩 $[0, 100]$
* 图像像素点 $[0, 255]$​

### 均值方差归一化

把所有的数据归一到平均值为0方差为1的分布中，适用于数据分布没有明显边界
$$
x_{\text{sacle}}=\frac{x-\mu}{\sigma}
$$

> [!warning]
>
> 如果数据没有明显的边界，一般都采用均值方差归一化方法。

### `Scaler`归一化工具

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/原始数据.png" style="zoom: 50%;" />

1. 真实数据无法获得均值和方差。
2. 采用均值方差归一化，要保留训练数据的均值和方差，用于处理预测数据。
3. 预测时，预测数据同样需要用测试数据的均值和方差归一化。

> [!warning]
>
> 对数据的归一化也可以理解为算法的一部分。

在sklearn中可以借助`Scaler`工具完成特征值归一化和均值方差保存的工作。

1. `StandardScaler`均值方差归一化

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X)
print(f'mean: {scaler.mean_}')
print(f'scale: {scaler.scale_}')
x_scaled = scaler.transform(X)
print(x_scaled)

```

1. `standardScaler.fit`函数可以对数据进行规划化，计算出均值和方差。
2. `standardScaler.mean_`和`standardScaler.scale_`计算出的均值和方差。
3. `standardScaler.transform`对训练数据和测试数据进行规一化处理。

绘制归一化后的汽车销量数据

```python
plt.figure(figsize=(10, 8))
plt.scatter(x_scaled.reshape(1, -1), y, s=120)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.show()
```

> [!tip]
>
> sklearn的最值归一化封装在[`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)中。

### sklearn中的sgd

使用sklearn中的[SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)，随机梯度下降算法，需要将特征归一化，否则计算出的数据会有偏差。

```python
from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(max_iter=1000, tol=1e-6, penalty=None, eta0=0.01, learning_rate='constant', random_state=42)
sgd_reg.fit(x_scaled, y)

print(sgd_reg.intercept_, sgd_reg.coef_)

coef_sgd = sgd_reg.coef_[0] / scaler.scale_[0]
intercept_sgd = y.mean() - coef_sgd * x.mean()

print("---------------------------")
print(f"coef_: {coef_sgd:.5f}")
print(f"intercept_: {intercept_sgd:.5f}")
```

归一化后数据的分布图。使用梯度下降法前，数据最好归一化。

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/1*dGXqtJOKa_Tbvt9nL3H7KQ.jpeg" style="zoom:65%;" />

> [!warning]
>
> 理论上可以证明，归一化数据不影响分类结果，但可以加快学习速率。梯度下降法对大数据量的训练有速度优势。

使用数据集测试SGD算法，导入数据划分测试集和训练集

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split

diabetes = datasets.load_diabetes()
x_train, x_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42)
print(x_train.shape)
print(x_test.shape)
print(x_train[0:1])
```

`scaled`默认值对数据进行归一化，使用`SGDRegressor`训练和测试数据

```python
sgd_reg = SGDRegressor(max_iter=1000, tol=1e-6, penalty=None, eta0=0.01, learning_rate='constant', random_state=42)
sgd_reg.fit(x_train, y_train)
print(sgd_reg.score(x_test, y_test))
```

## 梯度的调试

导数的定义如下
$$
\frac{df(x)}{dx}=\lim_{h\rightarrow0}\frac{f(x+h)-f(x)}{h}
$$
在计算机中模拟导数的计算采取如下公式
$$
\frac{dJ}{d\theta}=\frac{J(\theta+\epsilon)-J(x-\epsilon )}{2\epsilon }
$$
当$\theta=\left( \theta_0, \theta_1, …,\theta_n \right)$计算梯度的公式有
$$
\frac{dJ}{d\theta_0}=\frac{J(\theta_0^+)-J(\theta_0^- )}{2\epsilon }
\qquad
\begin{cases}
\theta_0^+=\left( \theta_0+\epsilon, \theta_1, …,\theta_n \right) \\
\theta_0^-=\left( \theta_0-\epsilon, \theta_1, …,\theta_n \right)
\end{cases}
$$
使用python代码来调试上面梯度算法的结果，生成测试数据如下

```python
np.random.seed(666)
X = np.random.random(size=(1000, 10))
true_theta = np.arange(1, 12, dtype=float)
X_b = np.hstack([np.ones((len(X), 1)), X])
y = X_b.dot(true_theta)
print(X.shape)
print(y.shape)
print(true_theta)
```

定义损失函数

```python
def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
    except:
        return float('inf')
```

使用矩阵运算的梯度计算

```python
def dJ_math(theta, X_b, y):
    return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(y)
```

使用求导公式的梯度计算

```python
def dJ_debug(theta, X_b, y, epsilon=0.01):
    res = np.empty(len(theta))
    for i in range(len(theta)):
        theta_1 = theta.copy()
        theta_1[i] += epsilon
        theta_2 = theta.copy()
        theta_2[i] -= epsilon
        res[i] = (J(theta_1, X_b, y) - J(theta_2, X_b, y)) / (2 * epsilon)
    return res
```

梯度下降法计算

```python
def gradient_descent(dJ, X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
    theta = initial_theta
    cur_iter = 0
    while cur_iter < n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon:
            break
        cur_iter += 1
    return theta
```

初始化计算参数

```python
X_b = np.hstack([np.ones((len(X), 1)), X])
initial_theta = np.zeros(X_b.shape[1])
eta = 0.01
```

使用矩阵运算的梯度下降法

```python
theta_debug = gradient_descent(dJ_debug, X_b, y, initial_theta, eta)
print(theta_debug)
```

使用求导的梯度下降法

```python
theta_math = gradient_descent(dJ_math, X_b, y, initial_theta, eta)
print(theta_math)
```

## 关于梯度下降法

* 批量梯度下降法（Batch Gradient Descent）
* 随机梯度下降法（Stohastic Gradient Descent）
* 小批量梯度下降法（Mini-Batch Gradient Descent）

<img src="https://raw.githubusercontent.com/hughxusu/lesson-ai/develop/images/base/1*bKSddSmLDaYszWllvQ3Z6A.jpg" style="zoom:75%;" />

随机计算在机器学习领域的优势主要表现在

* 跳出局部最优解
* 更快的运算速度
